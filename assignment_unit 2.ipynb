{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a25678c9993c1c2bbf0167f2ff03c982",
     "grade": false,
     "grade_id": "header-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Tips\n",
    "- To avoid unpleasant surprises, I suggest you _run all cells in their order of appearance_ (__Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "\n",
    "- If the changes you've made to your solution don't seem to be showing up, try running __Kernel__ $\\rightarrow$ __Restart & Run All__ from the menu.\n",
    "\n",
    "\n",
    "- Before submitting your assignment, make sure everything runs as expected. First, restart the kernel (from the menu, select __Kernel__ $\\rightarrow$ __Restart__) and then **run all cells** (from the menu, select __Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "## Reminder\n",
    "\n",
    "- Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name, UA email, and collaborators below:\n",
    "\n",
    "\n",
    "\n",
    "Several of the cells in this notebook are **read only** to ensure instructions aren't unintentionally altered.  \n",
    "\n",
    "If you can't edit the cell, it is probably intentional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Kathleen Costa\"\n",
    "# University of Arizona email address\n",
    "EMAIL = \"kathleencosta@arizona.edu\"\n",
    "# Names of any collaborators.  Write N/A if none.\n",
    "COLLABORATORS = \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0783621da2f047c6360f2ec0d56f121c",
     "grade": false,
     "grade_id": "cell-e35b85c2416e40f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Scratchpad\n",
    "\n",
    "You are welcome to create new cells (see the __Cell__ menu) to experiment and debug your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c80ac423030cfa372644d7cd456061af",
     "grade": false,
     "grade_id": "cell-955f8133afe96b26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "776264c84ca2ffc29ee3695115f9945e",
     "grade": false,
     "grade_id": "cell-a2292c2fbc4cf52e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Mini Python tutorial\n",
    "\n",
    "This course uses Python 3.11.\n",
    "\n",
    "Below is a very basic (and incomplete) overview of the Python language... \n",
    "\n",
    "For those completely new to Python, [this section of the official documentation may be useful](https://docs.python.org/3.11/library/stdtypes.html#common-sequence-operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3eb435ae7649ab2e5de50f02ff27fd26",
     "grade": false,
     "grade_id": "cell-d6593132353238c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      "[2, 3, 4, 5]\n",
      "[2]\n",
      "hello, Josuke!\n",
      "Howdy, partner!\n",
      "13\n",
      "Hi, Fred!\n",
      "[('radical', 4), ('analysis', 7), ('bighorn', 12), ('bounce', 32)]\n",
      "[('analysis', 7), ('bighorn', 12), ('bounce', 32), ('radical', 4)]\n"
     ]
    }
   ],
   "source": [
    "# This is a comment.  \n",
    "# Any line starting with # will be interpreted as a comment\n",
    "\n",
    "# this is a string assigned to a variable\n",
    "greeting = \"hello\"\n",
    "\n",
    "# If enclosed in triple quotes, strings can also be multiline:\n",
    "\n",
    "\"\"\"\n",
    "I'm a multiline\n",
    "string.\n",
    "\"\"\"\n",
    "\n",
    "# let's use a for loop to print it letter by letter\n",
    "for letter in greeting:\n",
    "    print(letter)\n",
    "    \n",
    "# Did you notice the indentation there?  Whitespace matters in Python!\n",
    "\n",
    "# here's a list of integers\n",
    "\n",
    "numbers = [1, 2, 3, 4]\n",
    "\n",
    "# let's add one to each number using a list comprehension\n",
    "# and assign the result to a variable called res\n",
    "# list comprehensions are used widely in Python (they're very Pythonic!)\n",
    "\n",
    "res = [num + 1 for num in numbers]\n",
    "\n",
    "# let's confirm that it worked\n",
    "print(res)\n",
    "\n",
    "# now let's try spicing things up using a conditional to filter out all values greater than or equal to 3...\n",
    "print([num for num in res if not num >= 3])\n",
    "\n",
    "# Python 3.7 introduced \"f-strings\" as a convenient way of formatting strings using templates\n",
    "# For example ...\n",
    "name = \"Josuke\"\n",
    "\n",
    "print(f\"{greeting}, {name}!\")\n",
    "\n",
    "# f-strings are f-ing convenient!\n",
    "\n",
    "\n",
    "# let's look at defining functions in Python..\n",
    "\n",
    "def greet(name):\n",
    "    print(f\"Howdy, {name}!\")\n",
    "\n",
    "# here's how we call it...\n",
    "\n",
    "greet(\"partner\")\n",
    "\n",
    "# let's add a description of the function...\n",
    "\n",
    "def greet(name):\n",
    "    \"\"\"\n",
    "    Prints a greeting given some name.\n",
    "    \n",
    "    :param name: the name to be addressed in the greeting\n",
    "    :type name: str\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"Howdy, {name}!\")\n",
    "    \n",
    "# I encourage you to use docstrings!\n",
    "\n",
    "# Python introduced support for optional type hints in v3.5.\n",
    "# You can read more aobut this feature here: https://docs.python.org/3.7/library/typing.html\n",
    "# let's give it a try...\n",
    "def add_six(num: int) -> int:\n",
    "    return num + 6\n",
    "\n",
    "# this should print 13\n",
    "print(add_six(7))\n",
    "\n",
    "# Python also has \"anonymous functions\" (also known as \"lambda\" functions)\n",
    "# take a look at the following code:\n",
    "\n",
    "greet_alt = lambda name: print(f\"Hi, {name}!\")\n",
    "\n",
    "greet_alt(\"Fred\")\n",
    "\n",
    "# lambda functions are often passed to other functions\n",
    "# For example, they can be used to specify how a sequence should be sorted\n",
    "# let's sort a list of pairs by their second element\n",
    "pairs = [(\"bounce\", 32), (\"bighorn\", 12), (\"radical\", 4), (\"analysis\", 7)]\n",
    "# -1 is last thing in some sequence, -2 is the second to last thing in some seq, etc.\n",
    "print(sorted(pairs, key=lambda pair: pair[-1]))\n",
    "\n",
    "# we can sort it by the first element instead\n",
    "# NOTE: python indexing is zero-based\n",
    "print(sorted(pairs, key=lambda pair: pair[0]))\n",
    "\n",
    "# You can learn more about other core data types and their methods here: \n",
    "# https://docs.python.org/3.7/library/stdtypes.html\n",
    "\n",
    "# Because of its extensive standard library, Python is often described as coming with \"batteries included\".  \n",
    "# Take a look at these \"batteries\": https://docs.python.org/3.7/library/\n",
    "\n",
    "# You now know enough to complete this homework assignment (or at least where to look)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed8f2b4d70b956550e58fa9741baf78b",
     "grade": false,
     "grade_id": "cell-c6c4eb790c878f19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Getting started\n",
    "\n",
    "In this assignment, you'll be implementing a simplistic and incomplete rule-based system for POS tagging. \n",
    "\n",
    "In cases where training data is available, part-of-speech tagging is typically performed using statistical methods.  While this assignment is intended to help you review token attributes, there are situations when rule-based approaches are still used.  For example, ... \n",
    "\n",
    "- When little annotated data is available for training statistical models\n",
    "- To relabel data using a different annotation scheme (ex. Penn $\\rightarrow$ UPOS)\n",
    "- To simplify the annotation task for humans by automatically labeling a portion of the data\n",
    "- etc.\n",
    "\n",
    "Can you think of other cases where a rule-based approach might be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07bb703f293852985209cee6e2ea023c",
     "grade": false,
     "grade_id": "cell-4d807c38cd18620e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Sequence, Text, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bfcdd18f5101bbb1c79e067dc475885",
     "grade": false,
     "grade_id": "cell-7d58546d5093c359",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Defining our `Sentence` class \n",
    "\n",
    "We'll use a Python class to keep track of tokens and their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5dc8d18b241736a8a64697718ed4e9f",
     "grade": false,
     "grade_id": "cell-749061fb4ad4c5f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sentence:\n",
    "    # Used to represent unknown symbols\n",
    "    UNKNOWN: Text = \"???\"\n",
    "    \"\"\"\n",
    "    Class representing a Sentence's tokens and their attributes.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokens: Sequence[Text],\n",
    "        norms: Optional[Sequence[Text]] = None,\n",
    "        pos: Optional[Sequence[Text]] = None\n",
    "    ):\n",
    "        # tokens\n",
    "        # NOTE: Tuple[Text, ...] means a tuple (i.e., an immutable sequence) \n",
    "        # of variable length where each element is a string (Text)\n",
    "        self.tokens: Tuple[Text, ...]   = tuple(tokens)\n",
    "        # normalized forms of each token\n",
    "        self.norms: Tuple[Text, ...]    = tuple(norms) if norms else tokens[::]\n",
    "        # part-of-speech tags\n",
    "        self.pos: Tuple[Text, ...]      = tuple(pos) if pos else tuple([Sentence.UNKNOWN] * self.size)\n",
    "        # ensure each token has an attribute of each type\n",
    "        assert all(self.size == len(attr) for attr in [self.pos, self.norms])\n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "        Calculates the number of tokens in our sentence.\n",
    "        \n",
    "        # Example: \n",
    "        s = Sentence(tokens=[\"I\", \"like\", \"turtles\"])\n",
    "        s.size == 3\n",
    "        \"\"\"\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Calculates the number of tokens in our sentence.\n",
    "        \n",
    "        # Example: \n",
    "        s = Sentence(tokens=[\"I\", \"like\", \"turtles\"])\n",
    "        len(s) == 3\n",
    "        \"\"\"\n",
    "        return self.size\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        The text displayed when printing an instance of our sentence.\n",
    "        \"\"\"\n",
    "        # convenience function to join lists\n",
    "        to_str = lambda elems: \"\\t\".join(elems)\n",
    "        return f\"\"\"\n",
    "        tokens:           {to_str(self.tokens)}\n",
    "        normalize tokens: {to_str(self.norms)}\n",
    "        pos:              {to_str(self.pos)}\n",
    "        \"\"\"\n",
    "    \n",
    "    def copy(self, \n",
    "        tokens = None, \n",
    "        norms = None,\n",
    "        pos = None):\n",
    "        \"\"\"\n",
    "        Convenience method to copy a Sentence and replace one or more of its attributes.\n",
    "        \"\"\"\n",
    "        return Sentence(\n",
    "            tokens   = tokens or self.tokens[::],\n",
    "            norms    = norms or self.norms[::],\n",
    "            pos      = pos or self.pos[::]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e51d6e1ef3b0a96c6ffe21ca27195604",
     "grade": false,
     "grade_id": "cell-e833e756b23bf1cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's practice using the `Sentence` class.  We'll create a sentence from the tokens `[\"I\", \"ate\", \"the\", \"muffin\"]` and assign `VBD` (past tense verb) to the token `ate` to create a new sentence instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "958905cf2bcac68b3ab7ee626d55cfd5",
     "grade": false,
     "grade_id": "cell-4cf3cb8368f26c68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           I\tate\tthe\tmuffin\n",
      "        normalize tokens: I\tate\tthe\tmuffin\n",
      "        pos:              ???\t???\t???\t???\n",
      "        \n",
      "\n",
      "        tokens:           I\tate\tthe\tmuffin\n",
      "        normalize tokens: I\tate\tthe\tmuffin\n",
      "        pos:              ???\tVBD\t???\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "s = Sentence(tokens = [\"I\", \"ate\", \"the\", \"muffin\"])\n",
    "print(s)\n",
    "\n",
    "s2 = s.copy(pos = [Sentence.UNKNOWN, \"VBD\", Sentence.UNKNOWN, Sentence.UNKNOWN])\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4616d133273b6e47aa1a38bc37810394",
     "grade": false,
     "grade_id": "cell-840dc33431f44d52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Implement a rule-based part of speech tagger\n",
    "\n",
    "We're going to implement a simplistic and incomplete rule-based part of speech tagger for English.  To further solidify what you learn, feel free to complete or extend it on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7a840c67eb06945a62640d51e723f64",
     "grade": false,
     "grade_id": "q1-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `rule_based_ly_adv_tagger(s)`\n",
    "\n",
    "`rule_based_ly_adv_tagger(s)` is a function designed to take a `Sentence` as input and rewrite the POS tag for any token ending in _ly_ as `RB` to produce a new `Sentence` as output.\n",
    "\n",
    "- Add you solution after the comment **YOUR CODE HERE** and remove the `raise NotImplementedError`.  \n",
    "- Do **not** use the `re` module!\n",
    "\n",
    "This rule will fail in certain cases.  Can you think of any way to improve it?  What are some adverbs will it fail to find altogether?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "957aba382bf275bde152786c9c073ba0",
     "grade": false,
     "grade_id": "q1-problem",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def rule_based_ly_adv_tagger(s: Sentence) -> Sentence:\n",
    "    \"\"\"\n",
    "    Takes a Sentence and returns a new Sentence with updated POS tags.\n",
    "    \n",
    "    If a token ends with \"ly\", assign it a POS tag of RB.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    updated_s = list(s.pos)\n",
    "    \n",
    "    for i, token in enumerate(s.tokens):\n",
    "        if token[-2:] == \"ly\":\n",
    "            updated_s[i] = \"RB\"  \n",
    "    \n",
    "    return s.copy(pos=tuple(updated_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f8ee855b2a887d73b6d3174673bb0d0",
     "grade": true,
     "grade_id": "q1-tests-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           I\t'm\tfairly\tcertain\tthis\twill\tbe\teasy\n",
      "        normalize tokens: I\t'm\tfairly\tcertain\tthis\twill\tbe\teasy\n",
      "        pos:              ???\t???\tRB\t???\t???\t???\t???\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens = [\"I\", \"'m\", \"fairly\", \"certain\", \"this\", \"will\", \"be\", \"easy\"])\n",
    "res = rule_based_ly_adv_tagger(s)\n",
    "print(res)\n",
    "assert res.pos == (UNK, UNK, \"RB\", UNK, UNK, UNK, UNK, UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d5a3a4e3312b49d3b9a43f362bbe0ea",
     "grade": true,
     "grade_id": "q1-tests-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           Do\tn't\tbe\tan\tugly\tbully\n",
      "        normalize tokens: Do\tn't\tbe\tan\tugly\tbully\n",
      "        pos:              ???\t???\t???\t???\tRB\tRB\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "# NOTE: this kind of naive rule-based tagger will make mistakes:\n",
    "s   = Sentence(tokens = [\"Do\", \"n't\", \"be\", \"an\", \"ugly\", \"bully\"])\n",
    "res = rule_based_ly_adv_tagger(s)\n",
    "# which tag(s) is/are wrong?\n",
    "print(res)\n",
    "assert res.pos == (UNK, UNK, UNK, UNK, \"RB\", \"RB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99ea0cbdfd899469846cb2f6b04c0eaa",
     "grade": true,
     "grade_id": "q1-tests-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s = Sentence(tokens=[\n",
    "    \"I\",\n",
    "    \"'m\", \n",
    "    \"anxiously\", \n",
    "    \"awaiting\", \n",
    "    \"your\", \n",
    "    \"answer\"\n",
    "])\n",
    "assert rule_based_ly_adv_tagger(s).pos == (Sentence.UNKNOWN, Sentence.UNKNOWN, \"RB\", Sentence.UNKNOWN, Sentence.UNKNOWN, Sentence.UNKNOWN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f1534fff182da96867a0b57de2587b1",
     "grade": false,
     "grade_id": "q2-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `rule_based_det_tagger(s)`\n",
    "\n",
    "`rule_based_det_tagger(s)` is a function designed to take a `Sentence` as input and rewrite the POS tag for any token ending that is one of the following words:\n",
    "\n",
    "```\n",
    "a, all, an, any, each, every, no, some, that, the, these, this, those, which\n",
    "```\n",
    "... as `DT`.\n",
    "\n",
    "- Add you solution after the comment **YOUR CODE HERE** and remove the `raise NotImplementedError`.  \n",
    "- Do **not** use the `re` module!\n",
    "- Ignore case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b252cd42b9b5d9d387f9f41240328653",
     "grade": false,
     "grade_id": "q2-problem",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def rule_based_det_tagger(s: Sentence) -> Sentence:\n",
    "    \"\"\"\n",
    "    Takes a Sentence and returns a new Sentence with updated POS tags.\n",
    "    \n",
    "    If a token is one of the following:\n",
    "      a, all, an, any, each, every, no, some, that, the, these, this, those, which\n",
    "    ... replace its tag with DT.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    updated_s = list(s.pos)\n",
    "    det_list = [\"a\", \"all\", \"an\", \"any\", \"each\", \"every\", \"no\",\"some\", \"that\", \"the\", \"these\", \"this\", \"those\",\"which\"]\n",
    "    \n",
    "    for i, word in enumerate(s.tokens):\n",
    "        if word.lower() in det_list:\n",
    "            updated_s [i] = \"DT\"\n",
    "    return s.copy(pos=tuple(updated_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f82c9e74de8686b0ce6a42fc7262ee53",
     "grade": true,
     "grade_id": "q2-tests-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           some\tcats\tLOVE\tthat\tfish\n",
      "        normalize tokens: some\tcats\tLOVE\tthat\tfish\n",
      "        pos:              DT\t???\t???\tDT\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens = [\"some\", \"cats\", \"LOVE\", \"that\", \"fish\"])\n",
    "res = rule_based_det_tagger(s)\n",
    "assert res.pos == (\"DT\", UNK, UNK, \"DT\", UNK)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ec3ef7ae9235d09a46fc38b42568128",
     "grade": true,
     "grade_id": "q2-tests-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           No\tmeans\tno\t,\tDr.\tNo\t!\n",
      "        normalize tokens: No\tmeans\tno\t,\tDr.\tNo\t!\n",
      "        pos:              DT\t???\tDT\t???\t???\tDT\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "# NOTE: While pretty reliable, this rule will occasionally make mistakes:\n",
    "s   = Sentence(tokens = [\"No\", \"means\", \"no\", \",\", \"Dr.\", \"No\", \"!\"])\n",
    "res = rule_based_det_tagger(s)\n",
    "# which tag(s) is/are wrong?\n",
    "print(res)\n",
    "assert res.pos == (\"DT\", UNK, \"DT\", UNK, UNK, \"DT\", UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20b248c7f65beb05f35b95553b11099d",
     "grade": true,
     "grade_id": "q2-tests-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s = Sentence(tokens=[\n",
    "    \"ALL\",\n",
    "    \"THE\", \n",
    "    \"YOUNG\", \n",
    "    \"DUDES\", \n",
    "    \"CARRY\", \n",
    "    \"THE\",\n",
    "    \"NEWS\"\n",
    "])\n",
    "assert rule_based_det_tagger(s).pos == (\"DT\", \"DT\", UNK, UNK, UNK, \"DT\", UNK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf7287920695882cd2f70e6de72e2a6d",
     "grade": false,
     "grade_id": "q3-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `rule_based_not_adv_adj_tagger(s)`\n",
    "\n",
    "So far we've written rules that function independently of one another.  That's very limiting.  One way we might improve our naive approach is to order the execution of our rules from highest confidence to lowest and have some rules depend on the output of others.  This time we'll write a rule that depends on the output of another.\n",
    "\n",
    "`rule_based_not_adv_adj_tagger(s)` is a function designed to take a `Sentence` as input and rewrite the POS tag to `JJ` for any token ending in a `y` that was not previously tagged as an adverb.\n",
    "\n",
    "- Add you solution after the comment **YOUR CODE HERE** and remove the `raise NotImplementedError`.  \n",
    "- Do **not** use the `re` module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04314508bab808170677c1901f763265",
     "grade": false,
     "grade_id": "q3-problem",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def rule_based_not_adv_adj_tagger(s: Sentence) -> Sentence:\n",
    "    \"\"\"\n",
    "    Takes a Sentence and returns a new Sentence with updated POS tags.\n",
    "    \n",
    "    If a token ends in y and is not already tagged as an adverb (RB), tag it as JJ.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    updated_s = list(s.pos)\n",
    "    \n",
    "    for i, token in enumerate(s.tokens):\n",
    "        if token[-2:] == \"ly\":\n",
    "            updated_s[i] = \"RB\"  \n",
    "        \n",
    "        elif token[-1:] == \"y\" and updated_s[i] != \"RB\":\n",
    "            updated_s[i] = \"JJ\"\n",
    "    \n",
    "    return s.copy(pos=tuple(updated_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd81707e202aae6f580ab6c4c406963c",
     "grade": true,
     "grade_id": "q3-tests-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           That\tmask\tis\tfairly\tscary\n",
      "        normalize tokens: That\tmask\tis\tfairly\tscary\n",
      "        pos:              ???\t???\t???\tRB\tJJ\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens=[\"That\", \"mask\", \"is\", \"fairly\", \"scary\"])\n",
    "res = rule_based_not_adv_adj_tagger(s)\n",
    "print(res)\n",
    "assert res.pos == (UNK, UNK, UNK, \"RB\", \"JJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90ec195302a04d0439ac52cba4d5ecbe",
     "grade": true,
     "grade_id": "q3-tests-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           Harry\tis\tvery\thairy\n",
      "        normalize tokens: Harry\tis\tvery\thairy\n",
      "        pos:              JJ\t???\tJJ\tJJ\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "# NOTE: of course, this tagger will still make mistakes:\n",
    "s   = Sentence(tokens=[\"Harry\", \"is\", \"very\", \"hairy\"])\n",
    "res = rule_based_not_adv_adj_tagger(s)\n",
    "# which tag(s) is/are wrong?\n",
    "print(res)\n",
    "assert res.pos == (\"JJ\", UNK, \"JJ\", \"JJ\")\n",
    "\n",
    "# How can we improve this further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa8851fd47ca182f259b6b8588f41fd8",
     "grade": false,
     "grade_id": "q4-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `verb_copula(s)` and `det_noun_verb(s)`\n",
    "\n",
    "Let's try to incorporate additional context in our rules.  We know that the surrounding tags can determine, constrain, or inform the tag assignment for the current word or token.\n",
    "\n",
    "`det_noun_verb(s)` is a function designed to take a `Sentence` as input and rewrite the POS tag to `NOUN` for any token immediately preceded by a token tagged as `DT` and immediately followed by a token that is tagged as some kind of verb.\n",
    "\n",
    "We'll also implement another function (`verb_copula`) to tag instances of the English \"be\" verb.\n",
    "\n",
    "- Add you solution after the comment **YOUR CODE HERE** and remove the `raise NotImplementedError`.  \n",
    "- Do **not** use the `re` module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db46d685819efd46d9770ad1f6095fca",
     "grade": false,
     "grade_id": "q4-problem",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def verb_copula(s: Sentence) -> Sentence:\n",
    "    \"\"\"\n",
    "    Takes a Sentence and returns a new Sentence with updated POS tags.\n",
    "    \n",
    "    Assigns the following tags:\n",
    "    \n",
    "    am -> VBP\n",
    "    is -> VBZ\n",
    "    are -> VBP\n",
    "    was -> VBD\n",
    "    were -> VBD\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    verbs = {\n",
    "        \"am\": \"VBP\",\n",
    "        \"is\": \"VBZ\",\n",
    "        \"are\": \"VBP\",\n",
    "        \"was\": \"VBD\",\n",
    "        \"were\": \"VBD\"\n",
    "    }\n",
    "\n",
    "    updated_pos = list(s.pos)\n",
    "\n",
    "    for i, token in enumerate(s.tokens):\n",
    "        if token.lower() in verbs:\n",
    "            updated_pos[i] = verbs[token.lower()]\n",
    "        else:\n",
    "            updated_pos[i] = Sentence.UNKNOWN  \n",
    "\n",
    "    return s.copy(pos=tuple(updated_pos))\n",
    "\n",
    "\n",
    "def det_noun_verb(s: Sentence) -> Sentence:\n",
    "    \"\"\"\n",
    "    Takes a Sentence and returns a new Sentence with updated POS tags.\n",
    "    \n",
    "    If a token _t_ is ...\n",
    "    1) immediately preceded by a token $_t_{t-1}$ already tagged as a determiner \n",
    "    and ...\n",
    "    2) immediately followed by a token already tagged as a verb\n",
    "    tag the token as a NOUN (we'll ignore plural vs singular distinctions).\n",
    "    \"\"\"\n",
    "    determiner_tag = \"DT\"\n",
    "    verbs_tags = {\"VBP\", \"VBZ\", \"VBD\", \"VBN\", \"VB\"}\n",
    "\n",
    "    updated_pos = list(s.pos)\n",
    "\n",
    "    for i in range(1, len(s.tokens) - 1):\n",
    "        if updated_pos[i - 1] == determiner_tag and updated_pos[i + 1] in verbs_tags:\n",
    "            updated_pos[i] = \"NOUN\"\n",
    "\n",
    "    return s.copy(pos=tuple(updated_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "153662d78898bbe4927f60105bcfe868",
     "grade": true,
     "grade_id": "q4-tests-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           I\tam\tthe\twalrus\n",
      "        normalize tokens: I\tam\tthe\twalrus\n",
      "        pos:              ???\tVBP\t???\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens=[\"I\", \"am\", \"the\", \"walrus\"])\n",
    "res = verb_copula(s)\n",
    "print(res)\n",
    "assert res.pos == (UNK, \"VBP\", UNK, UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ceb45f3503548ea3ddf20ddbcf9c764",
     "grade": true,
     "grade_id": "q4-tests-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           We\tare\tthe\tchampions\n",
      "        normalize tokens: We\tare\tthe\tchampions\n",
      "        pos:              ???\tVBP\t???\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens=[\"We\", \"are\", \"the\", \"champions\"])\n",
    "res = verb_copula(s)\n",
    "print(res)\n",
    "assert res.pos == (UNK, \"VBP\", UNK, UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19743618f69a080e9059a4663f05e9e9",
     "grade": true,
     "grade_id": "q4-tests-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           They\twere\tlate\n",
      "        normalize tokens: They\twere\tlate\n",
      "        pos:              ???\tVBD\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens=[\"They\", \"were\", \"late\"])\n",
    "res = verb_copula(s)\n",
    "print(res)\n",
    "assert res.pos == (UNK, \"VBD\", UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49228a9364669e801ca1194b3ce6f1f8",
     "grade": true,
     "grade_id": "q4-tests-4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           Who\twas\tsinging\t?\n",
      "        normalize tokens: Who\twas\tsinging\t?\n",
      "        pos:              ???\tVBD\t???\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens=[\"Who\", \"was\", \"singing\", \"?\"])\n",
    "res = verb_copula(s)\n",
    "print(res)\n",
    "assert res.pos == (UNK, \"VBD\", UNK, UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23630ccd69f48cc12942c8480fd82e51",
     "grade": true,
     "grade_id": "q4-tests-5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           The\tgoat\n",
      "        normalize tokens: The\tgoat\n",
      "        pos:              DT\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens=[\"The\", \"goat\"], pos=[\"DT\", UNK])\n",
    "res = det_noun_verb(s)\n",
    "print(res)\n",
    "assert res.pos == (\"DT\", UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e927d87ab1b224dec97fd39546199d64",
     "grade": true,
     "grade_id": "q4-tests-6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           The\tgoat\tdreams\n",
      "        normalize tokens: The\tgoat\tdreams\n",
      "        pos:              DT\tNOUN\tVBZ\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens=[\"The\", \"goat\", \"dreams\"], pos=[\"DT\", UNK, \"VBZ\"])\n",
    "res = det_noun_verb(s)\n",
    "print(res)\n",
    "assert res.pos == (\"DT\", \"NOUN\", \"VBZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29d109f7bf1d669e0021d1d1e6326732",
     "grade": true,
     "grade_id": "q4-tests-7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           The\tgoat\tdreams\n",
      "        normalize tokens: The\tgoat\tdreams\n",
      "        pos:              ???\t???\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens=[\"The\", \"goat\", \"dreams\"])\n",
    "res = det_noun_verb(s)\n",
    "print(res)\n",
    "assert res.pos == (UNK, UNK, UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78722c63f0e7bf23c971faa458a0c031",
     "grade": true,
     "grade_id": "q4-tests-8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           The\n",
      "        normalize tokens: The\n",
      "        pos:              DT\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# verify the re module is not in scope\n",
    "assert \"re\" not in dir()\n",
    "\n",
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens=[\"The\"], pos=[\"DT\"])\n",
    "res = det_noun_verb(s)\n",
    "print(res)\n",
    "assert res.pos == (\"DT\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2205c717700981c15e1a279d777c3fc0",
     "grade": false,
     "grade_id": "q5-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now that we have some rules, let's order them by priority to tag a few examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25d4d050875b62e91d1e7334c6dde78b",
     "grade": false,
     "grade_id": "q5-problem",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tag_with_rules(s: Sentence) -> Sentence:\n",
    "    \"\"\"\n",
    "    Takes a Sentence and returns a new Sentence with updates POS tags\n",
    "    by applying a series of rules for part of speech tagging\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    #you may notice comment lines between codes, those are not for any particular reason aside from my own organization\n",
    "    verbs = {\n",
    "        \"am\": \"VBP\",\n",
    "        \"is\": \"VBZ\",\n",
    "        \"are\": \"VBP\",\n",
    "        \"was\": \"VBD\",\n",
    "        \"were\": \"VBD\"\n",
    "    }\n",
    "    det_list = [\"a\", \"all\", \"an\", \"any\", \"each\", \"every\", \"no\",\"some\", \"that\", \"the\", \"these\", \"this\", \"those\",\"which\"]\n",
    "    \n",
    "    determiner_tag = \"DT\"\n",
    "    verbs_tags = {\"VBP\", \"VBZ\", \"VBD\", \"VBN\", \"VB\"}\n",
    "\n",
    "    updated_s = list(s.pos)\n",
    "    \n",
    "   \n",
    "    for i, token in enumerate(s.tokens):\n",
    "        token_lower = token.lower()\n",
    "        # # #\n",
    "        if token_lower in det_list:\n",
    "            updated_s[i] = \"DT\"\n",
    "        # # #\n",
    "        elif token_lower in verbs:\n",
    "            updated_s[i] = verbs[token_lower]\n",
    "        # # #\n",
    "        elif token_lower.endswith(\"ly\"):\n",
    "            updated_s[i] = \"RB\"\n",
    "        # # #\n",
    "        elif token_lower.endswith(\"y\"):\n",
    "            updated_s[i] = \"JJ\"\n",
    "        # # #  \n",
    "        for i in range(1, len(s.tokens) - 1):\n",
    "            if updated_s[i - 1] == determiner_tag and updated_s[i + 1] in verbs_tags:\n",
    "                updated_s[i] = \"NOUN\"\n",
    "        \n",
    "    return s.copy(pos=tuple(updated_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60c99e15767f65197cfacc32dfac4fdc",
     "grade": true,
     "grade_id": "q5-tests-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           Those\thungry\tgoats\tbleat\tquietly\n",
      "        normalize tokens: Those\thungry\tgoats\tbleat\tquietly\n",
      "        pos:              DT\tJJ\t???\t???\tRB\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens=[\"Those\", \"hungry\", \"goats\", \"bleat\", \"quietly\"])\n",
    "res = tag_with_rules(s)\n",
    "print(res)\n",
    "assert res.pos == (\"DT\", \"JJ\", UNK, UNK, \"RB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39fead9db62f032adc4317f493e07874",
     "grade": true,
     "grade_id": "q5-tests-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           Those\thungry\tgoats\tbleat\tquietly\n",
      "        normalize tokens: Those\thungry\tgoats\tbleat\tquietly\n",
      "        pos:              DT\tJJ\t???\tV??\tRB\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens=[\"Those\", \"hungry\", \"goats\", \"bleat\", \"quietly\"], pos=[UNK, UNK, UNK, \"V??\", \"RB\"])\n",
    "res = tag_with_rules(s)\n",
    "print(res)\n",
    "assert res.pos == (\"DT\", \"JJ\", UNK, \"V??\", \"RB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87cf84dc00fa25eac75ea08035a129ce",
     "grade": true,
     "grade_id": "q5-tests-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        tokens:           All\tbats\tare\trighteously\tfunky\n",
      "        normalize tokens: All\tbats\tare\trighteously\tfunky\n",
      "        pos:              DT\tNOUN\tVBP\tRB\tJJ\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "s   = Sentence(tokens=[\"All\", \"bats\", \"are\", \"righteously\", \"funky\"])\n",
    "res = tag_with_rules(s)\n",
    "print(res)\n",
    "assert res.pos == (\"DT\", \"NOUN\", \"VBP\", \"RB\", \"JJ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbc3c75cd34cef464b1f7983f16d097b",
     "grade": false,
     "grade_id": "conclusion",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Part of speech tagging is often treated as a preliminary step to many other NLP tasks (ex. shallow parsing, syntactic parsing, information extraction, etc.). In LING 539, you'll learn about statistical approaches to sequence tagging and train your own sequence tagger.  \n",
    "\n",
    "In situations where you have little to no existing annotated data to train statistical models, rule-based taggers can expedite the data annotation process by performing a \"first pass\" at partially annotating a dataset automatically.  As partial annotations are collected from human annotators, the rule-based system can be re-run to help fill in the blanks.  These systems can quickly grow in complexity.  Writing tests as you develop rules can help to ensure your system behaves as intended.\n",
    "\n",
    "What are some strengths and weaknesses of this method for assigning part of speech tags?  How can it be improved further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3162ac3140926fbdeaa16ab81b56703a",
     "grade": false,
     "grade_id": "bonus-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## BONUS (max 2 points; code + description)\n",
    "\n",
    "\n",
    "### Option 1: Improve your tagger\n",
    "\n",
    "\n",
    "You've now written a toy rule-based POS tagger.  Try improving it!  \n",
    "\n",
    "- You may copy your previous rules below and/or write entirely new ones. \n",
    "- If you're feeling confident, try further organizing your code using a class.\n",
    "  - ex. `class EnglishRuleBasedPosTagger` with a `def tag(self, s: Sentence) -> Sentence:` method.\n",
    "\n",
    "Write a few tests to demonstrate its capabilities.  \n",
    "- What are some of its shortcomings?  \n",
    "- How does it improve upon the earlier version?  \n",
    "- What tagset did you adopt?\n",
    "\n",
    "```python\n",
    "def tag_with_rules_v2(s: Sentence) -> Sentence:\n",
    "    \"\"\"\n",
    "    Takes a Sentence and returns a new Sentence with updates POS tags\n",
    "    by applying a series of rules for part of speech tagging\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "```\n",
    "\n",
    "### Option 2: Write a simple rule-based tagger for a non-English language\n",
    "\n",
    "In this unit, we looked at some ways part of speech tagging can differ across languages and finished up by building a simple a toy rule-based tagger for English.  Try applying what you've learned to build a rule-based tagger for a non-English language of your choice.\n",
    "\n",
    "- Write **at least 3** rules\n",
    "\n",
    "Write a few tests to demonstrate its capabilities.  \n",
    "- What are some of its shortcomings? \n",
    "- What tagset did you adopt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "992ad788bc1d369dd3d6f04c2d66b3b8",
     "grade": true,
     "grade_id": "cell-b9c5318ffe6315d7",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a22c26ec3d48c0300814bc5203ad796",
     "grade": false,
     "grade_id": "cell-c67e41e3656e6968",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_Please describe your implementation for the bonus problem in the cell below and address the questions presented in the bonus description. If you're writing a tagger for a non-English language, provide some background on the language._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eded2b7be8aafba75c59a2faf6227ae1",
     "grade": true,
     "grade_id": "cell-42ed19915217f2b3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
