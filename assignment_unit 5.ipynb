{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a25678c9993c1c2bbf0167f2ff03c982",
     "grade": false,
     "grade_id": "header-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Tips\n",
    "- To avoid unpleasant surprises, I suggest you _run all cells in their order of appearance_ (__Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "\n",
    "- If the changes you've made to your solution don't seem to be showing up, try running __Kernel__ $\\rightarrow$ __Restart & Run All__ from the menu.\n",
    "\n",
    "\n",
    "- Before submitting your assignment, make sure everything runs as expected. First, restart the kernel (from the menu, select __Kernel__ $\\rightarrow$ __Restart__) and then **run all cells** (from the menu, select __Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "## Reminder\n",
    "\n",
    "- Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name, UA email, and collaborators below:\n",
    "\n",
    "\n",
    "\n",
    "Several of the cells in this notebook are **read only** to ensure instructions aren't unintentionally altered.  \n",
    "\n",
    "If you can't edit the cell, it is probably intentional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Kathleen Costa\"\n",
    "# University of Arizona email address\n",
    "EMAIL = \"kathleencosta@arizona.edu\"\n",
    "# Names of any collaborators.  Write N/A if none.\n",
    "COLLABORATORS = \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0783621da2f047c6360f2ec0d56f121c",
     "grade": false,
     "grade_id": "cell-e35b85c2416e40f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Scratchpad\n",
    "\n",
    "You are welcome to create new cells (see the __Cell__ menu) to experiment and debug your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c80ac423030cfa372644d7cd456061af",
     "grade": false,
     "grade_id": "cell-955f8133afe96b26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "776264c84ca2ffc29ee3695115f9945e",
     "grade": false,
     "grade_id": "cell-a2292c2fbc4cf52e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Mini Python tutorial\n",
    "\n",
    "This course uses Python 3.11.\n",
    "\n",
    "Below is a very basic (and incomplete) overview of the Python language... \n",
    "\n",
    "For those completely new to Python, [this section of the official documentation may be useful](https://docs.python.org/3.11/library/stdtypes.html#common-sequence-operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3eb435ae7649ab2e5de50f02ff27fd26",
     "grade": false,
     "grade_id": "cell-d6593132353238c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      "[2, 3, 4, 5]\n",
      "[2]\n",
      "hello, Josuke!\n",
      "Howdy, partner!\n",
      "13\n",
      "Hi, Fred!\n",
      "[('radical', 4), ('analysis', 7), ('bighorn', 12), ('bounce', 32)]\n",
      "[('analysis', 7), ('bighorn', 12), ('bounce', 32), ('radical', 4)]\n"
     ]
    }
   ],
   "source": [
    "# This is a comment.  \n",
    "# Any line starting with # will be interpreted as a comment\n",
    "\n",
    "# this is a string assigned to a variable\n",
    "greeting = \"hello\"\n",
    "\n",
    "# If enclosed in triple quotes, strings can also be multiline:\n",
    "\n",
    "\"\"\"\n",
    "I'm a multiline\n",
    "string.\n",
    "\"\"\"\n",
    "\n",
    "# let's use a for loop to print it letter by letter\n",
    "for letter in greeting:\n",
    "    print(letter)\n",
    "    \n",
    "# Did you notice the indentation there?  Whitespace matters in Python!\n",
    "\n",
    "# here's a list of integers\n",
    "\n",
    "numbers = [1, 2, 3, 4]\n",
    "\n",
    "# let's add one to each number using a list comprehension\n",
    "# and assign the result to a variable called res\n",
    "# list comprehensions are used widely in Python (they're very Pythonic!)\n",
    "\n",
    "res = [num + 1 for num in numbers]\n",
    "\n",
    "# let's confirm that it worked\n",
    "print(res)\n",
    "\n",
    "# now let's try spicing things up using a conditional to filter out all values greater than or equal to 3...\n",
    "print([num for num in res if not num >= 3])\n",
    "\n",
    "# Python 3.7 introduced \"f-strings\" as a convenient way of formatting strings using templates\n",
    "# For example ...\n",
    "name = \"Josuke\"\n",
    "\n",
    "print(f\"{greeting}, {name}!\")\n",
    "\n",
    "# f-strings are f-ing convenient!\n",
    "\n",
    "\n",
    "# let's look at defining functions in Python..\n",
    "\n",
    "def greet(name):\n",
    "    print(f\"Howdy, {name}!\")\n",
    "\n",
    "# here's how we call it...\n",
    "\n",
    "greet(\"partner\")\n",
    "\n",
    "# let's add a description of the function...\n",
    "\n",
    "def greet(name):\n",
    "    \"\"\"\n",
    "    Prints a greeting given some name.\n",
    "    \n",
    "    :param name: the name to be addressed in the greeting\n",
    "    :type name: str\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"Howdy, {name}!\")\n",
    "    \n",
    "# I encourage you to use docstrings!\n",
    "\n",
    "# Python introduced support for optional type hints in v3.5.\n",
    "# You can read more aobut this feature here: https://docs.python.org/3.7/library/typing.html\n",
    "# let's give it a try...\n",
    "def add_six(num: int) -> int:\n",
    "    return num + 6\n",
    "\n",
    "# this should print 13\n",
    "print(add_six(7))\n",
    "\n",
    "# Python also has \"anonymous functions\" (also known as \"lambda\" functions)\n",
    "# take a look at the following code:\n",
    "\n",
    "greet_alt = lambda name: print(f\"Hi, {name}!\")\n",
    "\n",
    "greet_alt(\"Fred\")\n",
    "\n",
    "# lambda functions are often passed to other functions\n",
    "# For example, they can be used to specify how a sequence should be sorted\n",
    "# let's sort a list of pairs by their second element\n",
    "pairs = [(\"bounce\", 32), (\"bighorn\", 12), (\"radical\", 4), (\"analysis\", 7)]\n",
    "# -1 is last thing in some sequence, -2 is the second to last thing in some seq, etc.\n",
    "print(sorted(pairs, key=lambda pair: pair[-1]))\n",
    "\n",
    "# we can sort it by the first element instead\n",
    "# NOTE: python indexing is zero-based\n",
    "print(sorted(pairs, key=lambda pair: pair[0]))\n",
    "\n",
    "# You can learn more about other core data types and their methods here: \n",
    "# https://docs.python.org/3.7/library/stdtypes.html\n",
    "\n",
    "# Because of its extensive standard library, Python is often described as coming with \"batteries included\".  \n",
    "# Take a look at these \"batteries\": https://docs.python.org/3.7/library/\n",
    "\n",
    "# You now know enough to complete this homework assignment (or at least where to look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5928c956b3e8f7e9fc6b33dcfe9999b",
     "grade": false,
     "grade_id": "cell-f709ce43cf53a191",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Dict, Iterable, List, Text, Tuple, Union\n",
    "from collections import Counter\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from math import isclose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29cc73c6d5eeda8c31bf1552ce892219",
     "grade": false,
     "grade_id": "cell-1887ca4820987d24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this assignment, you will ... \n",
    "\n",
    "- implement a function to calculate prior probabilities\n",
    "- implement a function to calculate conditional probabilities\n",
    "- estimate the probability of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "298c1916818eaf0af95e54f10d4689d8",
     "grade": false,
     "grade_id": "cell-4766e335cf929823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# NumPy\n",
    "\n",
    "We'll be using [NumPy (**num**erical **Py**thon)](https://numpy.org/) to efficiently tally counts and generate probabilities.   While not part of the standard library, numpy is widely popular among Python users working in data science and machine learning.  If you're new to NumPy, be sure to watch the videos from this unit for an introduction to the library and some of its relevant features.  [You may also want to check out the official tutorial.](https://numpy.org/devdocs/user/absolute_beginners.html)\n",
    "\n",
    "NumPy is a very fast and efficient library for [manipulating vectors and matrices of numbers](https://en.wikipedia.org/wiki/Array_programming).  In this assignment, we'll just be scratching the surface of its capabilities.\n",
    "\n",
    "As a warm-up, complete the following functions using `NumPy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec34ceb55d8a52194029f7a277d4dec6",
     "grade": false,
     "grade_id": "cell-f64ddfe5927c4a02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `array_of_zeros(num_zeros: int)`\n",
    "\n",
    "Implement a method that creates a 1D vector of zeros based on the provided number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "878b46432e411e6cecdd301b4b2b3f4b",
     "grade": false,
     "grade_id": "cell-64824737a9852539",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def array_of_zeros(num_zeros: int) -> NDArray[float]:\n",
    "    \"\"\"\n",
    "    Creates a numpy array of zeros.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return np.zeros(num_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd5e7d06744f0dd0a1958f88e99a6fdf",
     "grade": true,
     "grade_id": "cell-0165074d6469f10d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# result should be a NumPy ndarray\n",
    "\n",
    "res = array_of_zeros(3)\n",
    "assert type(res) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e550211bf3a8f7944753f4fa3e14f256",
     "grade": true,
     "grade_id": "cell-5eb703988465d745",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# result should be a 1D array\n",
    "\n",
    "res = array_of_zeros(3)\n",
    "assert res.ndim == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae872f8d3a468204635736ffb3878110",
     "grade": true,
     "grade_id": "cell-f0de87c5573dbc7c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ensure returned array is composed of a sequence of length 3\n",
    "\n",
    "res = array_of_zeros(3)\n",
    "assert res.shape[0] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df89691c75934536d996ea4e167f17dd",
     "grade": true,
     "grade_id": "cell-a2acc227eeda350b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ensure all values are zeros\n",
    "\n",
    "res = array_of_zeros(3)\n",
    "assert all(x == 0 for x in res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eae4a6b59fc2223e579e48028f66478b",
     "grade": false,
     "grade_id": "cell-415df5a9b9072814",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `add_scalar(vector: NDArray[float], scalar: int)`\n",
    "\n",
    "Implement a method that adds a scalar value to each element in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b42da41ed7171ed07890b1d745b2ed45",
     "grade": false,
     "grade_id": "cell-b523e0903ece5de5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def add_scalar(vector: NDArray[float], scalar: int) -> NDArray[float]:\n",
    "    \"\"\"\n",
    "    Takes a NumPy Array and adds a scalar value to each element in the array\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return vector + scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c84de818b35fc55d6650d06f08c0c499",
     "grade": true,
     "grade_id": "cell-cc121bbc40eab257",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# result should be a NumPy ndarray\n",
    "\n",
    "res = add_scalar(array_of_zeros(7), 34)\n",
    "assert type(res) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "450744f051258b6902ddd3db49965b44",
     "grade": true,
     "grade_id": "cell-86df8187fded1fbe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ensure all elements of resulting array are equal to 2\n",
    "\n",
    "res = add_scalar(array_of_zeros(4), 2)\n",
    "assert all(x == 2 for x in res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e768681db57566f7cbf6c8fa81b76b71",
     "grade": true,
     "grade_id": "cell-e34078733eda9baf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ensure result is a 1D array\n",
    "\n",
    "res = add_scalar(array_of_zeros(4), 2)\n",
    "assert res.ndim == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae92b69fb415587e4982e91a8b6ef738",
     "grade": true,
     "grade_id": "cell-3f5371a4133cdd82",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ensure length of result is 4\n",
    "\n",
    "res = add_scalar(array_of_zeros(4), 2)\n",
    "assert res.shape[0] == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92082d615717bab27d6da14fbe8a9530",
     "grade": false,
     "grade_id": "cell-c92fd766dd7b1bd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `divide_by_scalar(vector: NDArray[float], scalar: int)`\n",
    "\n",
    "Implement a method that divides each element in a numpy array by a scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "214dd940f4f79a8bc39ff8e871c4979b",
     "grade": false,
     "grade_id": "cell-81fbec16e00dd608",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def divide_by_scalar(vector: NDArray[float], scalar: int) -> NDArray[float]:\n",
    "    \"\"\"\n",
    "    Takes a NumPy Array and divides each element in the array by a scalar value\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return vector / scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cf2e47dab04933950dbe47236db87e5",
     "grade": true,
     "grade_id": "cell-679e2c1e3ca8ec96",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "res = divide_by_scalar(add_scalar(array_of_zeros(14), 2), 17)\n",
    "assert type(res) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ce5d408c67fb43e6df1cf4509b96512",
     "grade": true,
     "grade_id": "cell-766d3ebd00c2bfa2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# all values should be 2\n",
    "res = divide_by_scalar(add_scalar(array_of_zeros(3), 4), 2)\n",
    "assert all(x == 2 for x in res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f666b7a3352367a329c224bf04683d7f",
     "grade": true,
     "grade_id": "cell-92c50e5c4d437c00",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ensure result is a 1D array\n",
    "res = divide_by_scalar(add_scalar(array_of_zeros(3), 4), 2)\n",
    "assert res.ndim == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5841126deda5de7942e4367efefa49f9",
     "grade": true,
     "grade_id": "cell-d005f674921d8eea",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "res = divide_by_scalar(add_scalar(array_of_zeros(300), 17), 14)\n",
    "assert res.shape[0] == 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2df90e4b7ae0baeff1c329c7b98d2dbd",
     "grade": false,
     "grade_id": "cell-4fe9e3e8fcac0fd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Constructing a Vocabulary\n",
    "\n",
    "Before generating conditional probabilities of higher order _n_-grams, we need to track and order the terms in our documents using a **Vocabulary** class.  The vocabulary class will help us determine which _n_-grams we want to consider and how to handle unseen terms.\n",
    "\n",
    "## `Vocabulary` class\n",
    "\n",
    "Though attribute and method names have changed, this is very similar to the class you implemented in Unit 4.  You may reuse your solution where possible.  Unlike the `Vocabulary` class you previously implemented, this version ...\n",
    "\n",
    "- is constructed from a sequence of tokens and optionally applies a count threshold when considering whether or not a term should be included in the vocabulary.\n",
    "- `id_for(self, term: str)` returns the ID for `Vocabulary.UKNOWN` if `term` is out of vocabulary (OOV).\n",
    "- includes a new method `empty_vector` for you to implement\n",
    "- includes an alternative constructor `from_sentences`.  This class-level method is already implemented.\n",
    "- requires you to filter the terms passed to `create_t2i` by frequency prior to constructing a dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1fa3e4860fc6b2fc525a3f87a0d791c",
     "grade": false,
     "grade_id": "cell-3d5f52ae2c4c386f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Stateful vocabulary.\n",
    "    Provides a mapping from term to ID and a reverse mapping of ID to term.\n",
    "    \"\"\"\n",
    "    # symbol for unknown terms    \n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    \n",
    "    def __init__(self, terms: Iterable[Text]=[], min_count: int = 1):\n",
    "        self.t2i: Dict[Text, int] = Vocabulary.create_t2i(terms, min_count=min_count)\n",
    "        self.i2t: Dict[int, Text] = Vocabulary.create_i2t(self.t2i)\n",
    "    \n",
    "    # see https://www.python.org/dev/peps/pep-0484/#forward-references\n",
    "    @staticmethod\n",
    "    def from_sentences(sentences: Iterable[Iterable[Text]], min_count: int = 1) -> \"Vocabulary\":\n",
    "        \"\"\"\n",
    "        Convenience method \n",
    "        for converting a sequence of tokenized sentences to a Vocabulary instance\n",
    "        \"\"\"\n",
    "        return Vocabulary(terms=[term for sentence in sentences for term in sentence], min_count=min_count)\n",
    "    \n",
    "    def id_for(self, term: Text) -> int:\n",
    "        \"\"\"\n",
    "        Looks up ID for term using self.t2i.  \n",
    "        If the feature is unknown, returns ID of Vocabulary.UNKNOWN.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return self.t2i.get(term, self.t2i[Vocabulary.UNKNOWN])\n",
    "        \n",
    "    def term_for(self, term_id: int) -> Union[Text, None]:\n",
    "        \"\"\"\n",
    "        Looks up term corresponding to term_id.  \n",
    "        If term_id is unknown, returns None.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return self.i2t.get(term_id, None)\n",
    "    \n",
    "    @property\n",
    "    def terms(self) -> List[Text]:\n",
    "        return [self.i2t[i] for i in range(len(self.i2t))]\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_t2i(terms: Iterable[Text], min_count: int = 1) -> Dict[Text, int]:\n",
    "        \"\"\"\n",
    "        Takes a flat iterable of terms (i.e., unigrams) and returns a dictionary of term -> int.\n",
    "        Assumes terms have already been normalized.\n",
    "        \n",
    "        If the frequency of a term is less than min_count, \n",
    "        do not include the term in the vocabulary\n",
    "        \n",
    "        Requirements:\n",
    "        - First term in vocabulary (ID 0) is reserved for Vocabulary.UNKNOWN.\n",
    "        - Sort the features alphabetically\n",
    "        - Only include terms occurring >= min_count\n",
    "        \"\"\"\n",
    "        # terms must be strings\n",
    "        if not all(isinstance(term, Text) for term in terms):\n",
    "            raise Exception(\"terms must be strings\")\n",
    "        # YOUR CODE HERE\n",
    "        term_counts = Counter(terms)\n",
    "        filtered_terms = sorted([term for term, count in term_counts.items() if count >= min_count])\n",
    "        t2i = {Vocabulary.UNKNOWN: 0}\n",
    "        \n",
    "        for index, term in enumerate(filtered_terms, start=1):\n",
    "            t2i[term] = index\n",
    "        return t2i\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_i2t(t2i: Dict[Text, int]) -> Dict[int, Text]:\n",
    "        \"\"\"\n",
    "        Takes a dict of str -> int and returns a reverse mapping of int -> str.\n",
    "        \"\"\"\n",
    "        return {i:t for (t, i) in t2i.items()}\n",
    "\n",
    "    def empty_vector(self) -> NDArray[float]:\n",
    "        \"\"\"\n",
    "        Creates an empty numpy array based on the vocabulary of terms\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return np.zeros(len(self.t2i), dtype=float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Defines what should happen when `len` is called on an instance of this class.\n",
    "        \"\"\"\n",
    "        return len(self.t2i)\n",
    "    \n",
    "    def __contains__(self, other):\n",
    "        \"\"\"\n",
    "        Example:\n",
    "        \n",
    "        v = Vocabulary([\"I\", \"am\"])\n",
    "        assert \"am\" in v\n",
    "        \"\"\"\n",
    "        return True if other in self.t2i else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "506508414df7b5077cc9c7bad3d6de38",
     "grade": true,
     "grade_id": "cell-ed5a3b490fc00ec7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"we\", \"are\", \"travelers\"],\n",
    "    [\"we\", \"are\", \"gamblers\"]\n",
    "]\n",
    "vocab = Vocabulary.from_sentences(sentences)\n",
    "res = vocab.empty_vector()\n",
    "\n",
    "# result should be a numpy array\n",
    "assert isinstance(res, np.ndarray)\n",
    "# everything should be a float\n",
    "assert all(isinstance(elem, float) for elem in res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca2394920dfe9fd0fc9387759c538b4f",
     "grade": true,
     "grade_id": "cell-8a9e69122153a37a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"we\", \"are\", \"travelers\"],\n",
    "    [\"we\", \"are\", \"gamblers\"]\n",
    "]\n",
    "vocab = Vocabulary.from_sentences(sentences, min_count=1)\n",
    "res = vocab.empty_vector()\n",
    "\n",
    "# vector should be a 1D array\n",
    "assert len(res.shape) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5974e679752f04c8f6ee0c4d64a98bc6",
     "grade": true,
     "grade_id": "cell-08a427622afb8601",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"we\", \"are\", \"travelers\"],\n",
    "    [\"we\", \"are\", \"gamblers\"]\n",
    "]\n",
    "vocab = Vocabulary.from_sentences(sentences, min_count=2)\n",
    "res = vocab.empty_vector()\n",
    "\n",
    "assert len(res) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f4b035dcf03792e8cac2661f3644266",
     "grade": true,
     "grade_id": "cell-f48df80a3df1c31b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"We\", \"are\", \"travelers\"],\n",
    "    [\"we\", \"are\", \"gamblers\"]\n",
    "]\n",
    "vocab = Vocabulary.from_sentences(sentences, min_count=2)\n",
    "res = vocab.empty_vector()\n",
    "\n",
    "assert len(res) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a867c690103ca05d7297b1a7c5dfccac",
     "grade": true,
     "grade_id": "cell-17be0d80f757f338",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"we\", \"are\", \"travelers\"],\n",
    "    [\"we\", \"are\", \"gamblers\"]\n",
    "]\n",
    "vocab = Vocabulary.from_sentences(sentences, min_count=4)\n",
    "res = vocab.empty_vector()\n",
    "\n",
    "assert len(res) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ee0de483d6962016b9f41a5cdf3c303",
     "grade": true,
     "grade_id": "cell-9395addde7813e1e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"we\", \"are\", \"travelers\"],\n",
    "    [\"we\", \"are\", \"gamblers\"]\n",
    "]\n",
    "vocab = Vocabulary.from_sentences(sentences, min_count=1)\n",
    "res = vocab.empty_vector()\n",
    "\n",
    "# dimensions of array should be equal to |V|\n",
    "assert res.shape == (len(vocab),)\n",
    "\n",
    "# |V| should be 5\n",
    "assert len(vocab) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d1016272f06c4dd45fa61797d679d8c",
     "grade": false,
     "grade_id": "cell-79672ca868780a41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Calclulate _n_-grams\n",
    "\n",
    "In order to calculate conditional probabilities, we'll first need to generate _n_-grams.\n",
    "\n",
    "## `ngrams_for(ngrams)`\n",
    "\n",
    "Generates a sequence of _n_-grams for the token sequence.  Each _n_-gram is represented as a tuple.\n",
    "\n",
    "**HINTS**:\n",
    "- You implemented this in Unit 4 and may reuse your solution here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36a8789b798766e94c76946c96ab45c0",
     "grade": false,
     "grade_id": "cell-0aaa545288385a8d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ngrams_for(\n",
    "    # the size of the n-gram\n",
    "    n: int, \n",
    "    # a list of tokens\n",
    "    tokens: List[Text], \n",
    "    # whether or not to use the start and end symbols\n",
    "    use_start_end: bool = True,\n",
    "    # the symbol to use for the start of the sequence (assuming user_start_end is true)\n",
    "    start_symbol: str = \"<S>\",\n",
    "    # the symbol to use for the end of the sequence (assuming user_start_end is true)\n",
    "    end_symbol: str = \"</S>\"\n",
    ") -> List[Tuple[str]]:\n",
    "    \"\"\"\n",
    "    Generates a list of n-gram tuples for the provided sequence of tokens.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if use_start_end:\n",
    "        tokens = [start_symbol] * (n - 1) + tokens + [end_symbol] * (n - 1)\n",
    "\n",
    "    ngrams = [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "464d3dd326783e61d273bf11b802e10c",
     "grade": false,
     "grade_id": "cell-2105b8faff6be8a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Calclulating prior probabilities\n",
    "\n",
    "Armed with a bit of NumPy, a `Vocabulary` class, and a means of generating _n_-grams, we can move on to calculating prior probabilties efficiently.\n",
    "\n",
    "## `prior_probs(ngrams: Iterable[Text], vocab: Vocabulary)`\n",
    "\n",
    "`prior_probs()` takes a sequence of terms (i.e., unigrams) and a `Vocabulary` instance as parameters.  It returns a probability distribution (1D numpy array of floats) containing all terms in the vocabulary.\n",
    "\n",
    "**HINTS**:\n",
    "- The length of the 1D array should equal the number of terms in the `Vocabulary` instance\n",
    "- Each value in the 1D array is a float ranging between 0 and 1\n",
    "- The sum of the 1D array should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c580eee800c985055817e8a9acde8da",
     "grade": false,
     "grade_id": "cell-c209ef0b7f0e1db0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def prior_probs(tokens: Iterable[str], vocab: Vocabulary) -> NDArray[float]:\n",
    "    \"\"\"\n",
    "    Calculates the prior probability for each token,\n",
    "    given some vocabulary\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    token_counts = Counter(tokens)\n",
    "    probabilities = np.zeros(len(vocab), dtype=float)\n",
    "    total_count = sum(token_counts.values())  # Total number of tokens\n",
    "\n",
    "    unknown_count = 0\n",
    "\n",
    "    for token, count in token_counts.items():\n",
    "        term_id = vocab.id_for(token)\n",
    "        if term_id == vocab.id_for(Vocabulary.UNKNOWN):\n",
    "            unknown_count += count  # Count unknown terms\n",
    "        else:\n",
    "            probabilities[term_id] = count / total_count\n",
    "\n",
    "    probabilities[vocab.id_for(Vocabulary.UNKNOWN)] = unknown_count / total_count\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3033183be56d5a9cc1a89db98d543e9e",
     "grade": true,
     "grade_id": "cell-f305c6e76cbd0a87",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens = [\":)\", \":)\", \":(\"]\n",
    "vocab  = Vocabulary(tokens)\n",
    "res    = prior_probs(tokens, vocab)\n",
    "\n",
    "# Our Vocabulary should have just two distinct terms plus Vocabulary.UNKNOWN\n",
    "assert len(res) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43ab9ee4bc5ec6936ee49b9120f2dac7",
     "grade": true,
     "grade_id": "cell-24b85a5c116f2814",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens = [\":)\", \":)\", \":(\"]\n",
    "vocab  = Vocabulary(tokens)\n",
    "res    = prior_probs(tokens, vocab)\n",
    "\n",
    "assert res[vocab.id_for(Vocabulary.UNKNOWN)] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "390876a719abb8da911d0f9f81a5e1e2",
     "grade": true,
     "grade_id": "cell-dba877e0b90c0fa2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens = [\":)\", \":)\", \":(\"]\n",
    "vocab  = Vocabulary(tokens)\n",
    "res    = prior_probs(tokens, vocab)\n",
    "\n",
    "assert res[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77a54ccd2e7d23bbeb0cddb58da4cde8",
     "grade": true,
     "grade_id": "cell-56bf80586019d732",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens = [\":)\", \":)\", \":(\"]\n",
    "vocab  = Vocabulary(tokens)\n",
    "res    = prior_probs(tokens, vocab)\n",
    "\n",
    "# It should not matter that our terms here use just punctuation,\n",
    "# if we have two of them out of three tokens, the prob is 2/3\n",
    "assert isclose(res[vocab.id_for(\":)\")], 0.666, abs_tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "284119f5acb191689b9dc7591d8fb751",
     "grade": true,
     "grade_id": "cell-63ab6d9fe1882779",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens = [\":)\", \":)\", \":(\"]\n",
    "vocab  = Vocabulary(tokens)\n",
    "res    = prior_probs(tokens, vocab)\n",
    "\n",
    "# result should form a probability distribution\n",
    "assert res.sum() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2501f4e4c016eda2bc2824fc50b359ed",
     "grade": true,
     "grade_id": "cell-4ef81504c14adb67",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens = [\":)\", \":)\", \":(\"]\n",
    "vocab  = Vocabulary(tokens)\n",
    "res    = prior_probs(tokens + [\"snorkel\"], vocab)\n",
    "\n",
    "# Our probability result vector should have places for just the two known terms\n",
    "# plus Vocabulary.UNKNOWN, just like our Vocabulary\n",
    "assert len(res) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4db3e5c70ddd6109408061b9736f5b97",
     "grade": true,
     "grade_id": "cell-41bad319e3f37c52",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens = [\":)\", \":)\", \":(\"]\n",
    "vocab  = Vocabulary(tokens)\n",
    "res    = prior_probs(tokens + [\"snorkel\", \"bike\", \"bike\"], vocab)\n",
    "\n",
    "# We have three unknown tokens out of six in our data\n",
    "# Be sure you're counting and adding them properly!\n",
    "assert res[0] == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b2435a62bdf17d78279fec6d6dbd683",
     "grade": true,
     "grade_id": "cell-9fa168a089c34609",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens = [\":)\", \":)\", \":(\"]\n",
    "vocab  = Vocabulary(tokens)\n",
    "res    = prior_probs(tokens + [\"snorkel\"], vocab)\n",
    "\n",
    "assert res[vocab.id_for(\"snorkel\")] == 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c6ee78d485620d44596207f95008c56",
     "grade": true,
     "grade_id": "cell-87c5b4304a16f31b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens = [\":)\", \":)\", \":(\"]\n",
    "vocab  = Vocabulary(tokens)\n",
    "res    = prior_probs(tokens + [\"snorkel\"], vocab)\n",
    "\n",
    "assert res[vocab.id_for(\":(\")] == 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "769be861f48894b391bf3f1e6675303b",
     "grade": true,
     "grade_id": "cell-90f677f65874f688",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens = [\":)\", \":)\", \":(\"]\n",
    "vocab  = Vocabulary(tokens)\n",
    "res    = prior_probs(tokens + [\"snorkel\"], vocab)\n",
    "\n",
    "# result should form a probability distribution\n",
    "assert res.sum() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e73169a6ec9ba1e308ae443ae66c9a6",
     "grade": false,
     "grade_id": "cell-8022d7995edf8868",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Calclulating conditional probabilities\n",
    "\n",
    "Now that you've practiced generating prior probabilities, it's time to explore conditional probabilities.\n",
    "\n",
    "## `make_conditional_probs(ngrams: Iterable[NgramType], vocab: Vocabulary)`\n",
    "\n",
    "`make_conditional_probs()` takes a sequence of _n_-grams and a `Vocabulary` instance as parameters.  It returns a dictionary of conditional probability distributions for each _n_-gram's history ($(\\text{the}, \\text{black}, \\text{dog}) \\rightarrow P(x \\vert \\text{the black}) = [, ..., ...]$).\n",
    "\n",
    "**HINTS**:\n",
    "- Each **key** in the dictionary is a **tuple** of observations\n",
    "- Each **value** in the dictionary is a **1D numpy array of floats**\n",
    "  - each array represents a probability distribution of outcomes for $x$ (i.e., the array should sum to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40b3d54fb3690a6023c78630d60a6e2d",
     "grade": false,
     "grade_id": "cell-04ca979ff499bffc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ex. [(\"I\", \"am\"), (\"she\", \"is\")]\n",
    "NgramType = Tuple[str, ...]\n",
    "\n",
    "def make_conditional_probs(ngrams: Iterable[NgramType], vocab: Vocabulary) -> Dict[Tuple[Text, ...], NDArray[float]]:\n",
    "    \"\"\"\n",
    "    Takes a sequence of n-grams and a vocabulary\n",
    "    Returns a dictionary of conditional probability distributions for each n-gram's history\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    history_counts = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        history = ngram[:-1] \n",
    "        outcome = ngram[-1]   \n",
    "\n",
    "        if history not in history_counts:\n",
    "            history_counts[history] = Counter()\n",
    "\n",
    "        history_counts[history][outcome] += 1\n",
    "\n",
    "    conditional_probs = {}\n",
    "\n",
    "    for history, outcome_counter in history_counts.items():\n",
    "        total_count = sum(outcome_counter.values())  \n",
    "        probabilities = np.zeros(len(vocab), dtype=float)  \n",
    "\n",
    "        for outcome, count in outcome_counter.items():\n",
    "            outcome_id = vocab.id_for(outcome)\n",
    "            probabilities[outcome_id] = count / total_count  \n",
    "\n",
    "        conditional_probs[history] = probabilities\n",
    "\n",
    "    return conditional_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5c24f0adae74e833bcad02828a7bc04",
     "grade": true,
     "grade_id": "cell-8d1f183a7d72649e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens  = [\"Sea\", \"slugs\", \"don\", \"'t\", \"sneeze\"]\n",
    "ngrams  = ngrams_for(n=2, tokens=tokens)\n",
    "vocab   = Vocabulary(tokens)\n",
    "res     = make_conditional_probs(ngrams=ngrams, vocab=vocab)\n",
    "\n",
    "# check return type\n",
    "assert isinstance(res, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff4207611a535be72843d5f4ee7b1aab",
     "grade": true,
     "grade_id": "cell-ac393d073b6a4de9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens  = [\"Sea\", \"slugs\", \"don\", \"'t\", \"sneeze\"]\n",
    "ngrams  = ngrams_for(n=2, tokens=tokens)\n",
    "vocab   = Vocabulary(tokens)\n",
    "res     = make_conditional_probs(ngrams=ngrams, vocab=vocab)\n",
    "\n",
    "# check return type\n",
    "assert all(isinstance(k, tuple) for k in res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afb8fc447a3caffbea2af9944eab4131",
     "grade": true,
     "grade_id": "cell-26bdccd5618ce489",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokens  = [\"Sea\", \"slugs\", \"don\", \"'t\", \"sneeze\"]\n",
    "ngrams  = ngrams_for(n=2, tokens=tokens)\n",
    "vocab   = Vocabulary(tokens)\n",
    "res     = make_conditional_probs(ngrams=ngrams, vocab=vocab)\n",
    "\n",
    "# check return type\n",
    "for v in res.values():\n",
    "    assert isinstance(v, np.ndarray), f\"{v} is not an npdarray\"\n",
    "    # everything should be a float\n",
    "    assert all(isinstance(elem, float) for elem in v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c86f49080b6119ce936712f78130440d",
     "grade": true,
     "grade_id": "cell-9a9882255c200611",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"I\", \"study\", \"turtles\"],\n",
    "    [\"I\", \"study\", \"cats\", \"and\", \"turtles\"],\n",
    "    [\"I\", \"can\", \"speak\", \"the\", \"language\", \"of\", \"turtles\"]\n",
    "]\n",
    "ngrams  = [gram for sent in sentences for gram in ngrams_for(n=2, tokens=sent)]\n",
    "vocab   = Vocabulary.from_sentences(sentences)\n",
    "res     = make_conditional_probs(ngrams=ngrams, vocab=vocab)\n",
    "\n",
    "# $p(can|I) \\approx 0.3333$\n",
    "assert isclose(res[(\"I\",)][vocab.id_for(\"can\")], 0.3333, abs_tol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5655cc76ef27fe6b920aa332ac964c10",
     "grade": true,
     "grade_id": "cell-b192d579e09028f4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"I\", \"study\", \"turtles\"],\n",
    "    [\"I\", \"study\", \"cats\", \"and\", \"turtles\"],\n",
    "    [\"I\", \"can\", \"speak\", \"the\", \"language\", \"of\", \"turtles\"]\n",
    "]\n",
    "ngrams  = [gram for sent in sentences for gram in ngrams_for(n=3, tokens=sent)]\n",
    "vocab   = Vocabulary.from_sentences(sentences)\n",
    "res     = make_conditional_probs(ngrams=ngrams, vocab=vocab)\n",
    "\n",
    "# $p(study|I study) \\approx 0.5$\n",
    "assert isclose(res[(\"I\", \"study\")][vocab.id_for(\"cats\")], 0.5, abs_tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6043a3d6052b6ad2111268efa1141d9",
     "grade": true,
     "grade_id": "cell-027610512695d9f8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"I\", \"study\", \"turtles\"],\n",
    "    [\"I\", \"study\", \"cats\", \"and\", \"turtles\"],\n",
    "    [\"I\", \"can\", \"speak\", \"the\", \"language\", \"of\", \"turtles\"]\n",
    "]\n",
    "ngrams  = [gram for sent in sentences for gram in ngrams_for(n=2, tokens=sent)]\n",
    "vocab   = Vocabulary.from_sentences(sentences)\n",
    "res     = make_conditional_probs(ngrams=ngrams, vocab=vocab)\n",
    "\n",
    "# $p(love|I) = 0$\n",
    "assert res[(\"I\",)][vocab.id_for(\"love\")] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9e1ff86f5b864cc3a63e8d1e863ee1b",
     "grade": true,
     "grade_id": "cell-c971e76d2ddbf601",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"I\", \"study\", \"turtles\"],\n",
    "    [\"I\", \"study\", \"cats\", \"and\", \"turtles\"],\n",
    "    [\"I\", \"can\", \"speak\", \"the\", \"language\", \"of\", \"turtles\"]\n",
    "]\n",
    "ngrams  = [gram for sent in sentences for gram in ngrams_for(n=2, tokens=sent)]\n",
    "vocab   = Vocabulary.from_sentences(sentences)\n",
    "res     = make_conditional_probs(ngrams=ngrams, vocab=vocab)\n",
    "\n",
    "# each value in the dictionary should form a probability distribution\n",
    "assert all(v.sum() == 1 for k,v in res.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa2c612e33cd1b7bf52792585f8d0379",
     "grade": false,
     "grade_id": "cell-4f20f282f47b85dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# _n_-gram language models\n",
    "\n",
    "Now that you know how to generate the probability of higher order _n_-grams, let's estimate the probability of sequences of tokens using an _n_-gram language model.  For more information on _n_-gram language models, [see Sections 3.0 - 3.1 of the textbook](https://parsertongue.org/readings/slp3/3.pdf#page=1).\n",
    "\n",
    "## `LanguageModel` class\n",
    "\n",
    "Implement a class that takes a corpus (`corpus`), determines the model's vocabulary, and computes the probability of text sequences using an $(n-1)$ order Markov assumption (i.e., estimate the probability of a sequence as the product of the probability of each of its _n_-grams for some value _n_). [Check the tutorial](https://parsertongue.org/tutorials/language-models-beginner/#markov-assumption) to refresh your memory on how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d76fad69538ac99bd5b21beb5cdc975c",
     "grade": false,
     "grade_id": "cell-975bc4ad96fb9e58",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LanguageModel():\n",
    "    \"\"\"\n",
    "    An _n_-gram language model using an _n_ - 1 order Markov assumption.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 corpus: Iterable[Iterable[str]], \n",
    "                 n=2,\n",
    "                 min_count=1,\n",
    "                 use_start_end: bool = False\n",
    "    ):\n",
    "        assert n >= 2\n",
    "        self.n = n\n",
    "        self.use_start_end = use_start_end\n",
    "        # though not stored as an instance attribute, we need this temporarily to calculate other attributes\n",
    "        ngrams: Iterable[Tuple(Text, ...)] = [gram for sentence in corpus\\\n",
    "                                             for gram in ngrams_for(n=self.n, tokens=sentence, use_start_end=self.use_start_end)]\n",
    "        self.vocab: Vocabulary                                   = Vocabulary.from_sentences(corpus, min_count)\n",
    "        self.pdist: Dict[Tuple[Text, ...], NDArray[float]]       = make_conditional_probs(ngrams, self.vocab)\n",
    "    \n",
    "    def cond_prob(self, term: Text, given: Tuple[str, ...]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the conditional probability for the provided term and the term's context.\n",
    "        \n",
    "        P(am|I) = cond_prob(term = \"am\", given = (\"I\",))\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        history = given\n",
    "        if history in self.pdist:\n",
    "            probabilities = self.pdist[history]\n",
    "            term_id = self.vocab.id_for(term)\n",
    "            return probabilities[term_id] if term_id < len(probabilities) else 0.0\n",
    "        return 0.0\n",
    "        \n",
    "    def prob_of(self, tokens: Iterable[Text]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the probability of a token sequence using an _n_ - 1 order Markov assumption.\n",
    "        \"\"\"\n",
    "        p = 1\n",
    "        for gram in ngrams_for(n=self.n, tokens=tokens, use_start_end=self.use_start_end):\n",
    "            next_tok = gram[-1]\n",
    "            history  = gram[:-1]\n",
    "        # YOUR CODE HERE\n",
    "        p = 1.0 \n",
    "        for gram in ngrams_for(n=self.n, tokens=tokens, use_start_end=self.use_start_end):\n",
    "            next_tok = gram[-1]\n",
    "            history = gram[:-1]\n",
    "            p *= self.cond_prob(next_tok, history) \n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2719bd38963979eddd1ddaff242c12c8",
     "grade": true,
     "grade_id": "cell-652704c59a87ae11",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lm = LanguageModel(\n",
    "    corpus=[\n",
    "        [\"I\", \"like\", \"turtles\"],\n",
    "        [\"I\", \"like\", \"horses\", \"and\", \"turtles\"]\n",
    "    ],\n",
    "    n=2\n",
    ")\n",
    "\n",
    "# In this naïve language model,\n",
    "# unseen terms/n-grams will result in 0 probabilities\n",
    "assert lm.prob_of([\"I\", \"like\", \"clowns\"]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4b46e9ad3bacdddc2f41c743daaba79",
     "grade": true,
     "grade_id": "cell-617d717ac74edf01",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lm = LanguageModel(\n",
    "    corpus=[\n",
    "        [\"I\", \"like\", \"noodles\"],\n",
    "        [\"I\", \"like\", \"dumplings\", \"and\", \"noodles\"]\n",
    "    ],\n",
    "    n=2\n",
    ")\n",
    "\n",
    "# In our tiny corpus, \"I\" is **always** followed by \"like\"\n",
    "assert lm.prob_of([\"I\", \"like\"]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "726f1f289acf1990238fd5992275768a",
     "grade": true,
     "grade_id": "cell-fce4f492486ce294",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lm = LanguageModel(\n",
    "    corpus=[\n",
    "        [\"I\", \"like\", \"noodles\"],\n",
    "        [\"I\", \"like\", \"dumplings\", \"and\", \"noodles\"]\n",
    "    ],\n",
    "    n=3\n",
    ")\n",
    "\n",
    "# In our tiny corpus, \"I like\" is followed by \"noodles\" half of the time\n",
    "assert  lm.prob_of([\"I\", \"like\", \"noodles\"]) == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50e4d442ff1e22c94e78148075a6b64a",
     "grade": true,
     "grade_id": "cell-366500abc0be5082",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lm = LanguageModel(\n",
    "    corpus=[\n",
    "        [\"I\", \"like\", \"noodles\"],\n",
    "        [\"I\", \"like\", \"dumplings\", \"and\", \"noodles\"]\n",
    "    ],\n",
    "    n=3\n",
    ")\n",
    "\n",
    "# In our tiny corpus, \"I like\" is followed by \"noodles\" half of the time\n",
    "assert  lm.prob_of([\"I\", \"like\", \"dumplings\"]) == 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f74f8a38c4646cda886a7fe5ed0c24a8",
     "grade": false,
     "grade_id": "cell-9b3d586358cea442",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Congratulations!  You've implemented an _n_-gram language model!   The examples we've looked at so far have involved toy datasets.  In order to get better estimates of probabilities, we need larger corpora.  Try training a bigram language model using a book from Project Gutenberg.  See below for an example to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from requests import get\n",
    "\n",
    "# url = \"http://www.gutenberg.org/cache/epub/35688/pg35688.txt\"\n",
    "\n",
    "# res = get(url)\n",
    "# content = res.text\n",
    "\n",
    "# # tokenize content, clean it up, and use it to train a bigram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af65a2f20a4c6de3dc7a58d368b5655b",
     "grade": false,
     "grade_id": "cell-260eae6d0aad9186",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Bonus:  Better estimates for probabilities (max 5 points)\n",
    "Our naïve language model assumes a probability of 0 when it encounters unknown terms.  A consequence of this is that grammatical strings end up being assigned a probability of zero.  \n",
    "\n",
    "Common solutions to this problem involve **smoothing** or a form of **backoff**.  \n",
    "\n",
    "### Task\n",
    "- Redefine the `LanguageModel` class and add a new method called `def smoothed_prob_of(self, tokens: Iterable[str]) -> float` and/or `def prob_of(self, tokens: Iterable[str], backoff: bool) -> float`.  \n",
    "- Implement a smoothing or backoff algorithm of your choice.  [See Sections 3.3-3.6 of the textbook for examples of smoothing and backoff algorithms](https://parsertongue.org/readings/slp3/3.pdf#page=9).  Alternatively, you may invent your own.\n",
    "- Add at least two tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fe124f1b469595f674c52d4af833e76",
     "grade": true,
     "grade_id": "cell-a1c0fff2f0cf76d1",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed probability of ['the', 'cat']: 0.21428571428571427\n",
      "Probability of ['the', 'cat'] with backoff: 0.5\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "class LanguageModel():\n",
    "    def __init__(self, \n",
    "                 corpus: Iterable[Iterable[str]], \n",
    "                 n=2,\n",
    "                 min_count=1,\n",
    "                 use_start_end: bool = False\n",
    "    ):\n",
    "        assert n >= 2\n",
    "        self.n = n\n",
    "        self.use_start_end = use_start_end\n",
    "        ngrams: Iterable[Tuple(Text, ...)] = [gram for sentence in corpus\n",
    "                                             for gram in ngrams_for(n=self.n, tokens=sentence, use_start_end=self.use_start_end)]\n",
    "        self.vocab: Vocabulary = Vocabulary.from_sentences(corpus, min_count)\n",
    "        self.pdist: Dict[Tuple[Text, ...], NDArray[float]] = make_conditional_probs(ngrams, self.vocab)\n",
    "    \n",
    "    def cond_prob(self, term: Text, given: Tuple[str, ...], smoothing: bool = False) -> float:\n",
    "        history = given\n",
    "        if history in self.pdist:\n",
    "            probabilities = self.pdist[history]\n",
    "            term_id = self.vocab.id_for(term)\n",
    "        if smoothing:\n",
    "                total_count = sum(probabilities)  \n",
    "                vocab_size = len(probabilities)\n",
    "                smoothed_prob = (probabilities[term_id] if term_id < len(probabilities) else 0) + 1  \n",
    "                return smoothed_prob / (total_count + vocab_size)  \n",
    "        else:\n",
    "                return probabilities[term_id] if term_id < len(probabilities) else 0.0\n",
    "        return 0.0  \n",
    "\n",
    "    def smoothed_prob_of(self, tokens: Iterable[str]) -> float:\n",
    "        p = 1.0  \n",
    "        for gram in ngrams_for(n=self.n, tokens=tokens, use_start_end=self.use_start_end):\n",
    "            next_tok = gram[-1]\n",
    "            history = gram[:-1]\n",
    "            p *= self.cond_prob(next_tok, history, smoothing=True)  # Use smoothing\n",
    "        return p\n",
    "\n",
    "    def prob_of(self, tokens: Iterable[str], backoff: bool = False) -> float:\n",
    "        p = 1.0  \n",
    "        for gram in ngrams_for(n=self.n, tokens=tokens, use_start_end=self.use_start_end):\n",
    "            next_tok = gram[-1]\n",
    "            history = gram[:-1]\n",
    "            prob = self.cond_prob(next_tok, history)\n",
    "\n",
    "            if backoff and prob == 0:\n",
    "                prob = self.cond_prob(next_tok, history[1:])  # Backoff to a smaller history\n",
    "            \n",
    "            p *= prob  \n",
    "        return p\n",
    "    \n",
    "    \n",
    "\n",
    "corpus = [[\"the\", \"cat\", \"purred\"], [\"the\", \"dog\", \"farted\"]]\n",
    "lm = LanguageModel(corpus, n=2)\n",
    "\n",
    "smoothed_prob = lm.smoothed_prob_of([\"the\", \"cat\"])\n",
    "print(f\"Smoothed probability of ['the', 'cat']: {smoothed_prob}\")\n",
    "\n",
    "backoff_prob = lm.prob_of([\"the\", \"cat\"], backoff=True)\n",
    "print(f\"Probability of ['the', 'cat'] with backoff: {backoff_prob}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain your approach here.\n",
    "\n",
    "The smoothing function helps to avoid categorizing unknown elements as 0. \"prob_of\" helps the model to identify a unigram from an unknown bigram. This allows the model to be more flexible in its labeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
