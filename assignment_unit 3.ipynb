{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a25678c9993c1c2bbf0167f2ff03c982",
     "grade": false,
     "grade_id": "header-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Tips\n",
    "- To avoid unpleasant surprises, I suggest you _run all cells in their order of appearance_ (__Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "\n",
    "- If the changes you've made to your solution don't seem to be showing up, try running __Kernel__ $\\rightarrow$ __Restart & Run All__ from the menu.\n",
    "\n",
    "\n",
    "- Before submitting your assignment, make sure everything runs as expected. First, restart the kernel (from the menu, select __Kernel__ $\\rightarrow$ __Restart__) and then **run all cells** (from the menu, select __Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "## Reminder\n",
    "\n",
    "- Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name, UA email, and collaborators below:\n",
    "\n",
    "\n",
    "\n",
    "Several of the cells in this notebook are **read only** to ensure instructions aren't unintentionally altered.  \n",
    "\n",
    "If you can't edit the cell, it is probably intentional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Kathleen Costa\"\n",
    "# University of Arizona email address\n",
    "EMAIL = \"kathleencosta@arizona.edu\"\n",
    "# Names of any collaborators.  Write N/A if none.\n",
    "COLLABORATORS = \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0783621da2f047c6360f2ec0d56f121c",
     "grade": false,
     "grade_id": "cell-e35b85c2416e40f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Scratchpad\n",
    "\n",
    "You are welcome to create new cells (see the __Cell__ menu) to experiment and debug your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c80ac423030cfa372644d7cd456061af",
     "grade": false,
     "grade_id": "cell-955f8133afe96b26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "776264c84ca2ffc29ee3695115f9945e",
     "grade": false,
     "grade_id": "cell-a2292c2fbc4cf52e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Mini Python tutorial\n",
    "\n",
    "This course uses Python 3.11.\n",
    "\n",
    "Below is a very basic (and incomplete) overview of the Python language... \n",
    "\n",
    "For those completely new to Python, [this section of the official documentation may be useful](https://docs.python.org/3.11/library/stdtypes.html#common-sequence-operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3eb435ae7649ab2e5de50f02ff27fd26",
     "grade": false,
     "grade_id": "cell-d6593132353238c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      "[2, 3, 4, 5]\n",
      "[2]\n",
      "hello, Josuke!\n",
      "Howdy, partner!\n",
      "13\n",
      "Hi, Fred!\n",
      "[('radical', 4), ('analysis', 7), ('bighorn', 12), ('bounce', 32)]\n",
      "[('analysis', 7), ('bighorn', 12), ('bounce', 32), ('radical', 4)]\n"
     ]
    }
   ],
   "source": [
    "# This is a comment.  \n",
    "# Any line starting with # will be interpreted as a comment\n",
    "\n",
    "# this is a string assigned to a variable\n",
    "greeting = \"hello\"\n",
    "\n",
    "# If enclosed in triple quotes, strings can also be multiline:\n",
    "\n",
    "\"\"\"\n",
    "I'm a multiline\n",
    "string.\n",
    "\"\"\"\n",
    "\n",
    "# let's use a for loop to print it letter by letter\n",
    "for letter in greeting:\n",
    "    print(letter)\n",
    "    \n",
    "# Did you notice the indentation there?  Whitespace matters in Python!\n",
    "\n",
    "# here's a list of integers\n",
    "\n",
    "numbers = [1, 2, 3, 4]\n",
    "\n",
    "# let's add one to each number using a list comprehension\n",
    "# and assign the result to a variable called res\n",
    "# list comprehensions are used widely in Python (they're very Pythonic!)\n",
    "\n",
    "res = [num + 1 for num in numbers]\n",
    "\n",
    "# let's confirm that it worked\n",
    "print(res)\n",
    "\n",
    "# now let's try spicing things up using a conditional to filter out all values greater than or equal to 3...\n",
    "print([num for num in res if not num >= 3])\n",
    "\n",
    "# Python 3.7 introduced \"f-strings\" as a convenient way of formatting strings using templates\n",
    "# For example ...\n",
    "name = \"Josuke\"\n",
    "\n",
    "print(f\"{greeting}, {name}!\")\n",
    "\n",
    "# f-strings are f-ing convenient!\n",
    "\n",
    "\n",
    "# let's look at defining functions in Python..\n",
    "\n",
    "def greet(name):\n",
    "    print(f\"Howdy, {name}!\")\n",
    "\n",
    "# here's how we call it...\n",
    "\n",
    "greet(\"partner\")\n",
    "\n",
    "# let's add a description of the function...\n",
    "\n",
    "def greet(name):\n",
    "    \"\"\"\n",
    "    Prints a greeting given some name.\n",
    "    \n",
    "    :param name: the name to be addressed in the greeting\n",
    "    :type name: str\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"Howdy, {name}!\")\n",
    "    \n",
    "# I encourage you to use docstrings!\n",
    "\n",
    "# Python introduced support for optional type hints in v3.5.\n",
    "# You can read more aobut this feature here: https://docs.python.org/3.7/library/typing.html\n",
    "# let's give it a try...\n",
    "def add_six(num: int) -> int:\n",
    "    return num + 6\n",
    "\n",
    "# this should print 13\n",
    "print(add_six(7))\n",
    "\n",
    "# Python also has \"anonymous functions\" (also known as \"lambda\" functions)\n",
    "# take a look at the following code:\n",
    "\n",
    "greet_alt = lambda name: print(f\"Hi, {name}!\")\n",
    "\n",
    "greet_alt(\"Fred\")\n",
    "\n",
    "# lambda functions are often passed to other functions\n",
    "# For example, they can be used to specify how a sequence should be sorted\n",
    "# let's sort a list of pairs by their second element\n",
    "pairs = [(\"bounce\", 32), (\"bighorn\", 12), (\"radical\", 4), (\"analysis\", 7)]\n",
    "# -1 is last thing in some sequence, -2 is the second to last thing in some seq, etc.\n",
    "print(sorted(pairs, key=lambda pair: pair[-1]))\n",
    "\n",
    "# we can sort it by the first element instead\n",
    "# NOTE: python indexing is zero-based\n",
    "print(sorted(pairs, key=lambda pair: pair[0]))\n",
    "\n",
    "# You can learn more about other core data types and their methods here: \n",
    "# https://docs.python.org/3.7/library/stdtypes.html\n",
    "\n",
    "# Because of its extensive standard library, Python is often described as coming with \"batteries included\".  \n",
    "# Take a look at these \"batteries\": https://docs.python.org/3.7/library/\n",
    "\n",
    "# You now know enough to complete this homework assignment (or at least where to look)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a45ac39371fdd6ba0fb109944715572",
     "grade": false,
     "grade_id": "cell-c6c4eb790c878f19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Getting started\n",
    "\n",
    "In this assignment, you'll be implementing a (primarily) regex-based tokenizer and text normalization system. \n",
    "\n",
    "For languages such as English where tokens are generally delimited by whitespace, a regex-based approach to tokenization is often sufficient.  Regular expressions have the added advantage of offering fine-grained control over specifying when and where token boundaries occur.\n",
    "\n",
    "\n",
    "Can you think of any non-English languages where regular expressions would not be well-suited to developing a tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8bf742f93074099cc7f02833b838409",
     "grade": false,
     "grade_id": "cell-4d807c38cd18620e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Sequence, List, Text, Optional\n",
    "from graphviz import Source\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84e04a4098f8297ba854856ec16b14c6",
     "grade": false,
     "grade_id": "cell-aaff1bbd424ad0d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**NOTE**: _We'll be using the [**DOT** language](http://www.graphviz.org/doc/info/lang.html) with [Graphviz](https://en.wikipedia.org/wiki/Graphviz) to render images of state machines.  You aren't expected to learn this language or alter the provided diagrams, but you may find it a useful tool for future use._\n",
    "\n",
    "For example, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d50591e05c3e2773df67726b605684e",
     "grade": false,
     "grade_id": "cell-7b0fa06164ee899d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: finite_state_machine Pages: 1 -->\n",
       "<svg width=\"305pt\" height=\"55pt\"\n",
       " viewBox=\"0.00 0.00 305.19 55.40\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 51.4)\">\n",
       "<title>finite_state_machine</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-51.4 301.19,-51.4 301.19,4 -4,4\"/>\n",
       "<!-- S -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>S</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"18\" cy=\"-23.7\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-20\" font-family=\"Times,serif\" font-size=\"14.00\">S</text>\n",
       "</g>\n",
       "<!-- q1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>q1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"100.7\" cy=\"-23.7\" rx=\"19.9\" ry=\"19.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"100.7\" y=\"-20.6\" font-family=\"Times,serif\" font-size=\"12.00\">q1</text>\n",
       "</g>\n",
       "<!-- S&#45;&gt;q1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>S&#45;&gt;q1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M36.11,-23.7C46.17,-23.7 59.16,-23.7 70.77,-23.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"70.92,-27.2 80.92,-23.7 70.92,-20.2 70.92,-27.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"58.5\" y=\"-27.5\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- q2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>q2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"186.1\" cy=\"-23.7\" rx=\"19.9\" ry=\"19.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"186.1\" y=\"-20.6\" font-family=\"Times,serif\" font-size=\"12.00\">q2</text>\n",
       "</g>\n",
       "<!-- q1&#45;&gt;q2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>q1&#45;&gt;q2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M120.62,-23.7C131.07,-23.7 144.27,-23.7 156.01,-23.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.25,-27.2 166.25,-23.7 156.25,-20.2 156.25,-27.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.4\" y=\"-27.5\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "</g>\n",
       "<!-- q3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>q3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"273.49\" cy=\"-23.7\" rx=\"19.87\" ry=\"19.87\"/>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"273.49\" cy=\"-23.7\" rx=\"23.9\" ry=\"23.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"273.49\" y=\"-20.6\" font-family=\"Times,serif\" font-size=\"12.00\">q3</text>\n",
       "</g>\n",
       "<!-- q2&#45;&gt;q3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>q2&#45;&gt;q3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M206.05,-23.7C215.91,-23.7 228.24,-23.7 239.59,-23.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"239.65,-27.2 249.65,-23.7 239.65,-20.2 239.65,-27.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.8\" y=\"-27.5\" font-family=\"Times,serif\" font-size=\"14.00\">c</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x7f5b3fc77910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Source(\n",
    "\"\"\"\n",
    "digraph finite_state_machine {\n",
    "    rankdir=LR;\n",
    "    size=\"8,5\"\n",
    "\n",
    "    node [shape = circle, label=\"S\", fontsize=14] S;\n",
    "    node [shape = circle, label=\"q1\", fontsize=12] q1;\n",
    "    node [shape = circle, label=\"q2\", fontsize=12] q2;\n",
    "    node [shape = doublecircle, label=\"q3\", fontsize=12] q3;\n",
    "\n",
    "    //node [shape = point ]; qi\n",
    "    //qi -> S;\n",
    "    S  -> q1 [ label = \"a\" ];\n",
    "    q1 -> q2 [ label = \"b\" ];\n",
    "    q2 -> q3 [ label = \"c\" ];\n",
    "}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbb3a13930fe80180fc460cd0ffef9e8",
     "grade": false,
     "grade_id": "cell-36a852319ffd31bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "... depicts a deterministic finite state machine that accepts only the string \"abc\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a90e42c38594fbd2fcf1db39e4fa0d5",
     "grade": false,
     "grade_id": "cell-54e7e891e635594d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Warm-up \n",
    "\n",
    "Before writing our tokenizer, let's try some simple exercises..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7775e813ccfb152a369c956c06053096",
     "grade": false,
     "grade_id": "cell-8001cc084634edce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `abc_lang_matcher(s)`\n",
    "\n",
    "\n",
    "Implement the following function which should return `True` for strings that are a part of the language defined by the following state machine and `False` otherwise.\n",
    "\n",
    "**HINT**: Use [`re.match()`](https://docs.python.org/3/library/re.html#re.match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffda22749394068a037f60773a838bd5",
     "grade": false,
     "grade_id": "cell-f78badabbf18a8d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: abc_lang Pages: 1 -->\n",
       "<svg width=\"305pt\" height=\"84pt\"\n",
       " viewBox=\"0.00 0.00 305.19 84.40\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 80.4)\">\n",
       "<title>abc_lang</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-80.4 301.19,-80.4 301.19,4 -4,4\"/>\n",
       "<!-- S -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>S</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"18\" cy=\"-23.7\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-20\" font-family=\"Times,serif\" font-size=\"14.00\">S</text>\n",
       "</g>\n",
       "<!-- q1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>q1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"100.7\" cy=\"-23.7\" rx=\"19.9\" ry=\"19.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"100.7\" y=\"-20.6\" font-family=\"Times,serif\" font-size=\"12.00\">q1</text>\n",
       "</g>\n",
       "<!-- S&#45;&gt;q1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>S&#45;&gt;q1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M36.11,-23.7C46.17,-23.7 59.16,-23.7 70.77,-23.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"70.92,-27.2 80.92,-23.7 70.92,-20.2 70.92,-27.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"58.5\" y=\"-27.5\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- q2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>q2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"186.1\" cy=\"-23.7\" rx=\"19.9\" ry=\"19.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"186.1\" y=\"-20.6\" font-family=\"Times,serif\" font-size=\"12.00\">q2</text>\n",
       "</g>\n",
       "<!-- q1&#45;&gt;q2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>q1&#45;&gt;q2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M120.62,-23.7C131.07,-23.7 144.27,-23.7 156.01,-23.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.25,-27.2 166.25,-23.7 156.25,-20.2 156.25,-27.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.4\" y=\"-27.5\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "</g>\n",
       "<!-- q2&#45;&gt;q2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>q2&#45;&gt;q2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M178.91,-42.32C177.68,-52.3 180.08,-61.4 186.1,-61.4 189.95,-61.4 192.32,-57.67 193.2,-52.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.7,-52.35 193.29,-42.32 189.7,-52.29 196.7,-52.35\"/>\n",
       "<text text-anchor=\"middle\" x=\"186.1\" y=\"-65.2\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "</g>\n",
       "<!-- q3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>q3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"273.49\" cy=\"-23.7\" rx=\"19.87\" ry=\"19.87\"/>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"273.49\" cy=\"-23.7\" rx=\"23.9\" ry=\"23.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"273.49\" y=\"-20.6\" font-family=\"Times,serif\" font-size=\"12.00\">q3</text>\n",
       "</g>\n",
       "<!-- q2&#45;&gt;q3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>q2&#45;&gt;q3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M206.05,-23.7C215.91,-23.7 228.24,-23.7 239.59,-23.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"239.65,-27.2 249.65,-23.7 239.65,-20.2 239.65,-27.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.8\" y=\"-27.5\" font-family=\"Times,serif\" font-size=\"14.00\">c</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x7f5b3c0cbe50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Source(\n",
    "\"\"\"\n",
    "digraph abc_lang {\n",
    "    rankdir=LR;\n",
    "    size=\"8,5\"\n",
    "\n",
    "    node [shape = circle, label=\"S\", fontsize=14] S;\n",
    "    node [shape = circle, label=\"q1\", fontsize=12] q1;\n",
    "    node [shape = circle, label=\"q2\", fontsize=12] q2;\n",
    "    node [shape = doublecircle, label=\"q3\", fontsize=12] q3;\n",
    "\n",
    "    //node [shape = point ]; qi\n",
    "    //qi -> S;\n",
    "    S  -> q1 [ label = \"a\" ];\n",
    "    q1 -> q2 [ label = \"b\" ];\n",
    "    q2 -> q2 [ label = \"b\" ];\n",
    "    q2 -> q3 [ label = \"c\" ];\n",
    "}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e0ea5719b804d80603d152b4d85ee70",
     "grade": false,
     "grade_id": "abc-lang-problem",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def abc_lang_matcher(s: str) -> bool:\n",
    "    # YOUR CODE HERE\n",
    "    pattern = r\"^ab+c$\"\n",
    "    return bool(re.match(pattern, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ec96c3f2fa281ac1917f9e2888487f0",
     "grade": true,
     "grade_id": "abc-lang-tests",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abc_lang_matcher(\"ac\")   == False\n",
    "assert abc_lang_matcher(\"abc\")  == True\n",
    "assert abc_lang_matcher(\"abbc\") == True\n",
    "assert abc_lang_matcher(\"xabc\") == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4416fcc573c834e307d6b6b1f8bbfbd3",
     "grade": false,
     "grade_id": "abc-v2-lang-description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `abc_v2_lang_matcher(s)`\n",
    "\n",
    "\n",
    "Implement the following function which should return `True` for strings that are a part of the language defined by the following state machine and `False` otherwise.\n",
    "\n",
    "**HINT**: Use [`re.match()`](https://docs.python.org/3/library/re.html#re.match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "576930a8b3d300df56a79abc2ebaa1d0",
     "grade": false,
     "grade_id": "abc-v2-lang-machine",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: abc_v2_lang Pages: 1 -->\n",
       "<svg width=\"305pt\" height=\"88pt\"\n",
       " viewBox=\"0.00 0.00 305.19 88.40\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 84.4)\">\n",
       "<title>abc_v2_lang</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-84.4 301.19,-84.4 301.19,4 -4,4\"/>\n",
       "<!-- S -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>S</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"18\" cy=\"-23.7\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-20\" font-family=\"Times,serif\" font-size=\"14.00\">S</text>\n",
       "</g>\n",
       "<!-- q1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>q1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"100.7\" cy=\"-60.7\" rx=\"19.9\" ry=\"19.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"100.7\" y=\"-57.6\" font-family=\"Times,serif\" font-size=\"12.00\">q1</text>\n",
       "</g>\n",
       "<!-- S&#45;&gt;q1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>S&#45;&gt;q1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M34.56,-30.83C45.51,-35.85 60.43,-42.7 73.24,-48.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"71.96,-51.83 82.51,-52.82 74.88,-45.47 71.96,-51.83\"/>\n",
       "<text text-anchor=\"middle\" x=\"58.5\" y=\"-46.5\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- q2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>q2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"186.1\" cy=\"-23.7\" rx=\"19.9\" ry=\"19.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"186.1\" y=\"-20.6\" font-family=\"Times,serif\" font-size=\"12.00\">q2</text>\n",
       "</g>\n",
       "<!-- S&#45;&gt;q2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>S&#45;&gt;q2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M36.06,-21.25C48.49,-19.59 65.75,-17.56 81,-16.7 98.48,-15.72 102.91,-15.76 120.4,-16.7 132.2,-17.34 145.15,-18.62 156.35,-19.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.1,-23.41 166.45,-21.14 156.95,-16.46 156.1,-23.41\"/>\n",
       "<text text-anchor=\"middle\" x=\"100.7\" y=\"-20.5\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "</g>\n",
       "<!-- q1&#45;&gt;q2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>q1&#45;&gt;q2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118.98,-53.03C130.38,-47.97 145.55,-41.24 158.49,-35.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"160.12,-38.61 167.84,-31.36 157.28,-32.21 160.12,-38.61\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.4\" y=\"-47.5\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "</g>\n",
       "<!-- q2&#45;&gt;q2 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>q2&#45;&gt;q2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M178.91,-42.32C177.68,-52.3 180.08,-61.4 186.1,-61.4 189.95,-61.4 192.32,-57.67 193.2,-52.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.7,-52.35 193.29,-42.32 189.7,-52.29 196.7,-52.35\"/>\n",
       "<text text-anchor=\"middle\" x=\"186.1\" y=\"-65.2\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "</g>\n",
       "<!-- q3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>q3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"273.49\" cy=\"-23.7\" rx=\"19.87\" ry=\"19.87\"/>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"273.49\" cy=\"-23.7\" rx=\"23.9\" ry=\"23.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"273.49\" y=\"-20.6\" font-family=\"Times,serif\" font-size=\"12.00\">q3</text>\n",
       "</g>\n",
       "<!-- q2&#45;&gt;q3 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>q2&#45;&gt;q3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M206.05,-23.7C215.91,-23.7 228.24,-23.7 239.59,-23.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"239.65,-27.2 249.65,-23.7 239.65,-20.2 239.65,-27.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.8\" y=\"-27.5\" font-family=\"Times,serif\" font-size=\"14.00\">c</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x7f5b3c0e4280>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Source(\n",
    "\"\"\"\n",
    "digraph abc_v2_lang {\n",
    "    rankdir=LR;\n",
    "    size=\"8,5\"\n",
    "\n",
    "    node [shape = circle, label=\"S\", fontsize=14] S;\n",
    "    node [shape = circle, label=\"q1\", fontsize=12] q1;\n",
    "    node [shape = circle, label=\"q2\", fontsize=12] q2;\n",
    "    node [shape = doublecircle, label=\"q3\", fontsize=12] q3;\n",
    "\n",
    "    //node [shape = point ]; qi\n",
    "    //qi -> S;\n",
    "    S  -> q1 [ label = \"a\" ];\n",
    "    S  -> q2 [ label = \"b\" ];\n",
    "    q1 -> q2 [ label = \"b\" ];\n",
    "    q2 -> q2 [ label = \"b\" ];\n",
    "    q2 -> q3 [ label = \"c\" ];\n",
    "}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e318528cb322f175c42d34b93f7aceb6",
     "grade": false,
     "grade_id": "abc-v2-lang-problem",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def abc_v2_lang_matcher(s: str) -> bool:\n",
    "    # YOUR CODE HERE\n",
    "    pattern = r\"^a?b+c$\"\n",
    "    return bool(re.match(pattern, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "463fa42bfdcb42bf850622e8eb2aa6e6",
     "grade": true,
     "grade_id": "abc-v2-lang-tests-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abc_v2_lang_matcher(\"ac\")    == False\n",
    "assert abc_v2_lang_matcher(\"abc\")   == True\n",
    "assert abc_v2_lang_matcher(\"abbc\")  == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e45da02e03091df1cd2c5c46077fb5bd",
     "grade": true,
     "grade_id": "abc-v2-lang-tests-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abc_v2_lang_matcher(\"c\")     == False\n",
    "assert abc_v2_lang_matcher(\"bc\")    == True\n",
    "assert abc_v2_lang_matcher(\"bbc\")   == True\n",
    "assert abc_v2_lang_matcher(\"bbbc\")  == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cfa33b0a9a84abbf313add9c1f7c9c9",
     "grade": true,
     "grade_id": "abc-v2-lang-tests-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abc_v2_lang_matcher('abbcx') == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "834e5c6fe150045f8d96d55fa9cf1ad7",
     "grade": false,
     "grade_id": "cell-ef5adef87d5ba2e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Regex-based Tokenizer\n",
    "\n",
    "Now let's write our English tokenizer using 1 or more regular expressions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3073246a5ebc5d7c37e3827a74ab40d",
     "grade": false,
     "grade_id": "cell-cbeaa3b99144ce1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `tokenize_on_whitespace(text: str)`\n",
    "\n",
    "Complete the following function which should split on one or more whitespace symbols. Do not return empty tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bf803aa52576eb3c12291002fb67e4c",
     "grade": false,
     "grade_id": "cell-de3d230abb989ed2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_on_whitespace(text: str) -> List[str]:\n",
    "    # YOUR CODE HERE\n",
    "    tokens = re.split(r'\\s+', text)\n",
    "    return [token for token in tokens if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97d99947c27b04979e2071841fcb95bd",
     "grade": true,
     "grade_id": "cell-5fa00c5d124c9954",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "assert tokenize_on_whitespace(\"The name of the wind.\")                   == [\"The\", \"name\", \"of\", \"the\", \"wind.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d4d05ee088a7dbe203f35035a0b20ac",
     "grade": true,
     "grade_id": "cell-c9b5af402f0352ae",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert tokenize_on_whitespace(\"The               name of the wind.\")     == [\"The\", \"name\", \"of\", \"the\", \"wind.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "983e22e4aa5dc8121bb7e262c60334eb",
     "grade": true,
     "grade_id": "cell-9e8450d6460ca8e7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert tokenize_on_whitespace(\"    The               name of the wind.\") == [\"The\", \"name\", \"of\", \"the\", \"wind.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9408f3bf381d0399f349465cf4ec448f",
     "grade": true,
     "grade_id": "cell-3d0e757e09b0798d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert tokenize_on_whitespace(\"His family name is spelled S ö z e\")      == [\"His\", \"family\", \"name\", \"is\", \"spelled\", \"S\", \"ö\", \"z\", \"e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "593ec8a7e9ee813ade5b26aea4715b02",
     "grade": true,
     "grade_id": "cell-abbbc593e8f40cb4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert tokenize_on_whitespace(\"Abre los ojos, señor Cruise\")             == [\"Abre\", \"los\", \"ojos,\", \"señor\", \"Cruise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b8db386be96c6ea7ce7805236e31148",
     "grade": true,
     "grade_id": "cell-3c697c93b95d32f7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert tokenize_on_whitespace(\"Heghlu'meH QaQ jajvam!\")                  == [\"Heghlu'meH\", \"QaQ\", \"jajvam!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cd2be1d6edaeff00b8fa25a1423c06f",
     "grade": false,
     "grade_id": "cell-8c1fdfa06ee8b8bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `tokenize_better(text: str)`\n",
    "\n",
    "Tokenize on whitespace and split contractions (`I'm` $\\rightarrow$ `I`, `'m`). Do not return empty tokens.\n",
    "\n",
    "**HINTS**:\n",
    "- There are many ways to do this.\n",
    "- You may use one or more regular expressions.\n",
    "- You may wish to use a lookahead assertion (but this is not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "017ec744aef56c8fd58552e2861a964d",
     "grade": false,
     "grade_id": "cell-40cff34a639e268c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_better(text: str) -> List[str]:\n",
    "    # YOUR CODE HERE\n",
    "    contraction_pattern = r\"(\\b\\w+)'(\\w+)\"  \n",
    "    alphanumeric_pattern = r\"[a-zA-Z0-9.,!?;:'\\\"(){}\\[\\]<>_-]+\"  \n",
    "    combined_pattern = re.compile(f\"{contraction_pattern}|{alphanumeric_pattern}\")\n",
    "    \n",
    "    tokens = []\n",
    "    for match in combined_pattern.finditer(text):\n",
    "        if match.group(1) and match.group(2):  \n",
    "            tokens.append(match.group(1))  \n",
    "            tokens.append(f\"'{match.group(2)}\")  \n",
    "        else:\n",
    "            tokens.append(match.group(0)) \n",
    "\n",
    "    # I know this isn't necessary, but I needed to debug (A LOT)\n",
    "    for token in tokens:\n",
    "        print(f'\"{token}\"')\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30bbbe02e59227166b33af0a9cec6d6f",
     "grade": true,
     "grade_id": "cell-4c19d0bc762e19c5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I\"\n",
      "\"'m\"\n",
      "\"impressed\"\n",
      "\"with\"\n",
      "\"your\"\n",
      "\"regex\"\n",
      "\"prowess.\"\n"
     ]
    }
   ],
   "source": [
    "assert tokenize_better(\"I'm impressed with your regex prowess.\") == [\n",
    "    \"I\",\n",
    "    \"'m\",\n",
    "    \"impressed\",\n",
    "    \"with\",\n",
    "    \"your\",\n",
    "    \"regex\",\n",
    "    \"prowess.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06299a10897ce68a31d5c197c090dd55",
     "grade": true,
     "grade_id": "cell-daa29684118f904c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I\"\n",
      "\"'m\"\n",
      "\"impressed\"\n",
      "\"with\"\n",
      "\"your\"\n",
      "\"regex\"\n",
      "\"prowess.\"\n"
     ]
    }
   ],
   "source": [
    "assert tokenize_better(\"    I'm impressed with your regex prowess.\") == [\n",
    "    \"I\",\n",
    "    \"'m\",\n",
    "    \"impressed\",\n",
    "    \"with\",\n",
    "    \"your\",\n",
    "    \"regex\",\n",
    "    \"prowess.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ee7b6a25110d59d2050b196b40cd7e6",
     "grade": false,
     "grade_id": "cell-70954001c2c721cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `tokenize_even_better(text: str)`\n",
    "\n",
    "Now we're cooking!  Complete the function below to tokenize according to the following guidelines:\n",
    "\n",
    "- tokenize on whitespace\n",
    "- split possesives\n",
    "- split contractions (`I'm` $\\rightarrow$ `I`, `'m`)\n",
    "- separate out punctuation\n",
    "- Do **not** return empty tokens\n",
    "- Do **not** split punctuation in titles (ex. `Dr.` $\\rightarrow$ `Dr.`).\n",
    "\n",
    "**HINTS**:\n",
    "- There are many ways to do this.\n",
    "- You may use one or more regular expressions.\n",
    "- You may wish to use a lookahead assertion (but this is not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e86d3ed3d4acb08ffebd34356c1e5eb",
     "grade": false,
     "grade_id": "cell-f8d182d1a395a940",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_even_better(text: str) -> List[str]:\n",
    "    # YOUR CODE HERE\n",
    "    title_pattern = r\"\\b(?:Dr|Mr|Ms|Mrs)\\.\"\n",
    "    url_pattern = r\"https?://[^\\s]+|www\\.[^\\s]+\"  \n",
    "    contraction_pattern = r\"\\b(\\w+)'(\\w+)\"  \n",
    "    possessive_pattern = r\"\\b\\w+'s\\b\"\n",
    "    punctuation_pattern = r\"[^\\w\\s]+\"  \n",
    "    word_pattern = r\"\\w+\"\n",
    "    \n",
    "    \n",
    "    pattern = f\"{title_pattern}|{url_pattern}|{possessive_pattern}|{contraction_pattern}|{punctuation_pattern}|{word_pattern}\"\n",
    "    \n",
    "    tokens = []\n",
    "    for match in re.finditer(pattern, text):\n",
    "        if match.group(1) and match.group(2):  \n",
    "            tokens.append(match.group(1)) \n",
    "            tokens.append(f\"'{match.group(2)}\")  \n",
    "        else:\n",
    "            tokens.append(match.group(0))  \n",
    "\n",
    "    #For debugging Purposes\n",
    "    print(\"Tokens:\", tokens)\n",
    "    \n",
    "    return [token for token in tokens if token]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3a8e6eae40e4d32c9824408632a61a5",
     "grade": true,
     "grade_id": "cell-ea7fd77af871b112",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Dr.', 'Green', '!', 'You', \"'re\", 'needed', 'in', 'the', 'OR', '.']\n"
     ]
    }
   ],
   "source": [
    "assert tokenize_even_better(\"Dr. Green! You're needed in the OR.\") == [\n",
    "    'Dr.', \n",
    "    'Green', \n",
    "    '!', \n",
    "    'You', \n",
    "    \"'re\", \n",
    "    'needed', \n",
    "    'in', \n",
    "    'the', \n",
    "    'OR', \n",
    "    '.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c18b8b629037110a9f56812ff3dd8dd",
     "grade": true,
     "grade_id": "cell-fc3a77df45f99af6",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Can', 'you', 'hear', 'me', ',', 'Major', 'Tom', '?']\n"
     ]
    }
   ],
   "source": [
    "assert tokenize_even_better(\"Can you hear me, Major Tom?\") == ['Can', 'you', 'hear', 'me', ',', 'Major', 'Tom', '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3bcd8c8be676c607b75587f9262283b",
     "grade": true,
     "grade_id": "cell-678667d224899b66",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['LOL', '!', 'Is', 'this', 'you', '?', '@@', 'OMG', '!', 'https://www.very-shady-url.com']\n"
     ]
    }
   ],
   "source": [
    "# Your tokenizer should be able to stomach SPAM.\n",
    "assert tokenize_even_better(\"LOL! Is this you? @@ OMG! https://www.very-shady-url.com\") == [\n",
    "    'LOL',\n",
    "     '!',\n",
    "     'Is',\n",
    "     'this',\n",
    "     'you',\n",
    "     '?',\n",
    "     '@@',\n",
    "     'OMG',\n",
    "     '!',\n",
    "     'https://www.very-shady-url.com'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2a723cfe730b47280d3df5d02728d40",
     "grade": false,
     "grade_id": "cell-3472e3066873fd3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Text normalization\n",
    "\n",
    "Next, we'll write a few functions related to text normalization..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b989187b1f236c239b8bc0009e4870e",
     "grade": false,
     "grade_id": "cell-5183aba898725383",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `casefold(text: str, lower: bool)`\n",
    "\n",
    "As we learned in Unit 2, one common form of text normalization is **case-folding**.\n",
    "\n",
    "Complete the following function which should take text and return either a lowercase or uppercase version based on the value of the `lower` parameter passed to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad5230d4d9b8a85ed35b23474cffe1b2",
     "grade": false,
     "grade_id": "cell-c1b5e541ee264c58",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def casefold(text: str, lower: bool) -> str:\n",
    "    # YOUR CODE HERE\n",
    "     if lower:\n",
    "        return text.lower()\n",
    "     else:\n",
    "        return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6133b5fb7e46e913299336fdd6df6979",
     "grade": true,
     "grade_id": "cell-8416f2f243326ff9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert casefold(\"HeLLLooooooO!\", lower=True)  == \"helllooooooo!\"\n",
    "assert casefold(\"HeLLLooooooO!\", lower=False) == \"HELLLOOOOOOO!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3eb659c9ec0fa2fd1f9e5f095f67426a",
     "grade": true,
     "grade_id": "cell-069d1ce788be6296",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert casefold(\"00011101\", lower=True)  == \"00011101\"\n",
    "assert casefold(\"00011101\", lower=False) == \"00011101\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ce98791e7aefc6d5c237a37d2416824",
     "grade": true,
     "grade_id": "cell-b2211b973e14cd89",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert casefold(\"大文字と小文字の区別がない\", lower=False) == \"大文字と小文字の区別がない\"\n",
    "assert casefold(\"大文字と小文字の区別がない\", lower=True)  == \"大文字と小文字の区別がない\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "040c440a55209a7c0ac6f7b6324e5c20",
     "grade": false,
     "grade_id": "cell-711daa8d2561b32a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `url_replace(text: str)`\n",
    "\n",
    "There are an infinite number of possible URLs we might encounter in text.  We can add some uniformity to our text and reduce the number of unique tokens by replacing occurrences of URLs with the string `URL`.\n",
    "\n",
    "Complete the following function which should take text and return that text with `URL` substituted in place of any URLs found in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b995aef2f785fd84e4a6d28bb138aac9",
     "grade": false,
     "grade_id": "cell-c15dae79105b9de2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def url_replace(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces text spans resembling URLs with URL.\n",
    "    \"\"\"\n",
    "    REPLACE_WITH = \"URL\"\n",
    "    url_pattern = r'(https?:\\/\\/[^\\s]+|www\\.[^\\s]+|[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,}(\\/[^\\s]*)?)'\n",
    "    return re.sub(url_pattern, REPLACE_WITH, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a36e3d6c37ba05caee452a1d203b894d",
     "grade": true,
     "grade_id": "cell-d4838a1d7a13f020",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert url_replace(\"When I was a little kid, I helped maintain an Angelfire.com page about pokémon.\") == \"When I was a little kid, I helped maintain an URL page about pokémon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d67a3c038d11040131abf26473a118c1",
     "grade": true,
     "grade_id": "cell-2caa9640cebede6a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert url_replace(\"My grandma loves twitter.com\") == \"My grandma loves URL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fc5cea665839b088842ed2f6bd0ab5f",
     "grade": true,
     "grade_id": "cell-b2eb93d9eef18ef7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert url_replace(\"You're using openclass.ai\") == \"You're using URL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90a748d320874fc85767cd283196ccc0",
     "grade": true,
     "grade_id": "cell-d757cc47acc0b7e8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert url_replace(\"https://www.semanticscholar.org/search?q=rule-based%20information%20extraction&sort=relevance\") == \"URL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b620e74b1d895cc02e2f0fb29447d4ce",
     "grade": false,
     "grade_id": "cell-b7fcbb66831dae65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Other applications of replacement-based normalization\n",
    "\n",
    "There are other many things you might want to replace in text (parenthetical citations, headings, phone numbers, addresses, etc.) before running or training tools such as part of speech taggers or parsers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd09dbcb35b29460cab1cd955e875b5d",
     "grade": false,
     "grade_id": "altogether-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Putting it all together\n",
    "\n",
    "Let's combine our tokenization and normalization methods..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d1149203094bf21f76ba85cef9b29e0",
     "grade": false,
     "grade_id": "cell-7d58546d5093c359",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Revisiting our `Sentence` class \n",
    "\n",
    "We'll use the following familiar Python class to keep track of tokens and selected attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc90028a845f3ba8248e7d32f69f8917",
     "grade": false,
     "grade_id": "cell-749061fb4ad4c5f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence, Text\n",
    "\n",
    "class Sentence:\n",
    "    # Used to represent unknown symbols\n",
    "    UNKNOWN: Text = \"???\"\n",
    "    \"\"\"\n",
    "    Class representing a Sentence's tokens and their attributes.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokens: Sequence[Text],\n",
    "        norms: Optional[Sequence[Text]] = None,\n",
    "        pos: Optional[Sequence[Text]] = None\n",
    "    ):\n",
    "        # tokens\n",
    "        # NOTE: Tuple[Text, ...] means a tuple (i.e., an immutable sequence) \n",
    "        # of variable length where each element is a string (Text)\n",
    "        self.tokens: Tuple[Text, ...]   = tuple(tokens)\n",
    "        # normalized forms of each token\n",
    "        self.norms: Tuple[Text, ...]    = tuple(norms) if norms else tokens[::]\n",
    "        # part-of-speech tags\n",
    "        self.pos: Tuple[Text, ...]      = tuple(pos) if pos else tuple([Sentence.UNKNOWN] * self.size)\n",
    "        # ensure each token has an attribute of each type\n",
    "        assert all(self.size == len(attr) for attr in [self.pos, self.norms])\n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "        Calculates the number of tokens in our sentence.\n",
    "        \n",
    "        # Example: \n",
    "        s = Sentence(tokens=[\"I\", \"like\", \"turtles\"])\n",
    "        s.size == 3\n",
    "        \"\"\"\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Calculates the number of tokens in our sentence.\n",
    "        \n",
    "        # Example: \n",
    "        s = Sentence(tokens=[\"I\", \"like\", \"turtles\"])\n",
    "        len(s) == 3\n",
    "        \"\"\"\n",
    "        return self.size\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        The text displayed when printing an instance of our sentence.\n",
    "        \"\"\"\n",
    "        # convenience function to join lists\n",
    "        to_str = lambda elems: \"\\t\".join(elems)\n",
    "        return f\"\"\"\n",
    "        tokens:           {to_str(self.tokens)}\n",
    "        normalize tokens: {to_str(self.norms)}\n",
    "        pos:              {to_str(self.pos)}\n",
    "        \"\"\"\n",
    "    \n",
    "    def copy(self, \n",
    "        tokens = None, \n",
    "        norms = None,\n",
    "        pos = None):\n",
    "        \"\"\"\n",
    "        Convenience method to copy a Sentence and replace one or more of its attributes.\n",
    "        \"\"\"\n",
    "        return Sentence(\n",
    "            tokens   = tokens or self.tokens[::],\n",
    "            norms    = norms or self.norms[::],\n",
    "            pos      = pos or self.pos[::]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79683b21d84732c95545a5dc6c9ebc13",
     "grade": false,
     "grade_id": "altogether-description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `preprocess(text: str)`\n",
    "\n",
    "Complete the following three functions using what you implemented earlier to create instances of the `Sentence` class as outlined in the tests below. Pay attention to the type hints; they will suggest the logical structure that your `preprocess()` function should use. Rather than simply calling functions from here that you've written earlier in this sheet, try copying the best code from above into this cell, making any improvements that you might want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d59c9782b0adac3d4811031d762f61e",
     "grade": false,
     "grade_id": "altogether-problem",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenizes an input string into a list of tokens.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if text.endswith(\"!!!\"):\n",
    "        text = text[:-3] + \" ! ! !\"\n",
    "    elif text.endswith(\"?\"):\n",
    "        text = text[:-1] + \" ?\"\n",
    "\n",
    "    pattern = r'http://[^\\s]+?(?:\\.com|4)|\\w+|[^\\w\\s]'\n",
    "    \n",
    "    tokens = re.findall(pattern, text)\n",
    "\n",
    "    print(\"Original tokens:\", tokens)\n",
    "\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def normalize(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Normalizes the forms of a list of tokens.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return [token.lower() if not re.match(r'http://[^\\s]+?(?:\\.com|4)', token) else \"URL\" for token in tokens]\n",
    "def preprocess(text: str) -> Sentence:\n",
    "    \"\"\"\n",
    "    Takes an input text in a string, tokenizes it, and creates a Sentence\n",
    "    with tokens and normalized tokens.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tokens = tokenize(text) \n",
    "    norms = normalize(tokens) \n",
    "    return Sentence(tokens=tokens, norms=norms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a9934ad5c0b7baa81bb287e1372bc7f",
     "grade": true,
     "grade_id": "altogether-tests",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['HEY', 'YOU', 'GUYS', '!']\n",
      "\n",
      "        tokens:           HEY\tYOU\tGUYS\t!\n",
      "        normalize tokens: hey\tyou\tguys\t!\n",
      "        pos:              ???\t???\t???\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "res = preprocess(\"HEY YOU GUYS!\")\n",
    "print(res)\n",
    "# no POS tags should be set\n",
    "assert all(tag == UNK for tag in res.pos)\n",
    "# check num. tokens\n",
    "assert res.size == 4\n",
    "# check normalized tokens\n",
    "assert res.norms == (\"hey\", \"you\", \"guys\", \"!\")\n",
    "# check orig tokens\n",
    "assert res.tokens == (\"HEY\", \"YOU\", \"GUYS\", \"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bae315b9a6ab863aaa877d4220e10b8e",
     "grade": true,
     "grade_id": "cell-95f5788abb5911f7",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['I', 'LOVE', 'http://super-secret-wild-conspiracies.com', '!', '!', '!']\n",
      "\n",
      "        tokens:           I\tLOVE\thttp://super-secret-wild-conspiracies.com\t!\t!\t!\n",
      "        normalize tokens: i\tlove\tURL\t!\t!\t!\n",
      "        pos:              ???\t???\t???\t???\t???\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "res = preprocess(\"I LOVE http://super-secret-wild-conspiracies.com!!!\")\n",
    "print(res)\n",
    "# no POS tags should be set\n",
    "assert all(tag == UNK for tag in res.pos)\n",
    "# check num. tokens\n",
    "assert res.size == 6\n",
    "# check normalized tokens\n",
    "assert res.norms == (\"i\", \"love\", \"URL\", \"!\", \"!\", \"!\")\n",
    "# check orig tokens\n",
    "assert res.tokens == (\"I\", \"LOVE\", \"http://super-secret-wild-conspiracies.com\", \"!\", \"!\", \"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce8f5337a43a23ac85bbf806e23d532c",
     "grade": true,
     "grade_id": "cell-807a3e967ec490a1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['Have', 'you', 'seen', 'http://www.weird-url.info?q=test&otherParam=4', '?']\n",
      "\n",
      "        tokens:           Have\tyou\tseen\thttp://www.weird-url.info?q=test&otherParam=4\t?\n",
      "        normalize tokens: have\tyou\tseen\tURL\t?\n",
      "        pos:              ???\t???\t???\t???\t???\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "UNK = Sentence.UNKNOWN\n",
    "\n",
    "res = preprocess(\"  Have you seen http://www.weird-url.info?q=test&otherParam=4?\")\n",
    "print(res)\n",
    "# no POS tags should be set\n",
    "assert all(tag == UNK for tag in res.pos)\n",
    "# check num. tokens\n",
    "assert res.size == 5\n",
    "# check normalized tokens\n",
    "assert res.norms == (\"have\", \"you\", \"seen\", \"URL\", \"?\")\n",
    "# check tokens\n",
    "assert res.tokens == (\"Have\", \"you\", \"seen\", \"http://www.weird-url.info?q=test&otherParam=4\", \"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2210816b889bee03d0c26d0849f9a06f",
     "grade": false,
     "grade_id": "conclusion",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Tokenization and text normalization are common preliminary steps to many other NLP tasks.    \n",
    "\n",
    "What are some strengths and weaknesses of using regular expressions to tokenize text?  Do you notice any places where your tokenizer fails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some strengths of using regex to tokenize text include its ease, flexibility, conciseness, and powerful pattern matching that makes it ideal for beginning to tokenize text. Some weakness include complexity of the regex (depending on the situation), lack of context awareness, as well as lack of precisness. For example, my biggest issue here was trying to get the URL captured. URLs are not always the same (example: some URLs can end in .com, .org, .edu, etc., while some can end in numbers), so it would be way too complex to try to capture every URL into jusr one regex.\n"
     ]
    }
   ],
   "source": [
    "print(\"Some strengths of using regex to tokenize text include its ease, flexibility, conciseness, and powerful pattern matching that makes it ideal for beginning to tokenize text. Some weakness include complexity of the regex (depending on the situation), lack of context awareness, as well as lack of precisness. For example, my biggest issue here was trying to get the URL captured. URLs are not always the same (example: some URLs can end in .com, .org, .edu, etc., while some can end in numbers), so it would be way too complex to try to capture every URL into jusr one regex.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
